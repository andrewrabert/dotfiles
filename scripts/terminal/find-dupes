#!/usr/bin/env python3
import argparse
import collections
import hashlib
import json
import pathlib


def all_files(root):
    stack = [pathlib.Path(root)]
    files = []
    while stack:
        for path in stack.pop().iterdir():
            if path.is_file() and not path.is_symlink():
                files.append(path)
            elif path.is_dir():
                stack.append(path)
    return files


def sha256sum(path):
    hasher = hashlib.sha256()
    with open(path, 'rb') as handle:
        while data := handle.read(65536):
            hasher.update(data)
    return hasher.hexdigest()


def _pretty_json(data):
    return json.dumps(data, indent=2, sort_keys=True)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-u', '--unique', action='store_true')
    parser.add_argument('path', default=[''], nargs='*')
    args = parser.parse_args()

    files = set()
    for path in args.path:
        path = pathlib.Path(path)
        if path.is_dir():
            files.update(all_files(path))
        else:
            files.add(path)

    by_size = collections.defaultdict(set)
    for path in sorted(files):
        by_size[path.stat().st_size].add(path)

    hashes = collections.defaultdict(set)
    for paths in by_size.values():
        if len(paths) > 1 or args.unique:
            for path in paths:
                hashes[sha256sum(path)].add(path)

    if args.unique:
        unique = sorted(
            str(list(v)[0].absolute())
            for k, v in hashes.items()
            if len(v) == 1
        )
        print(_pretty_json(unique))
    else:
        duplicates = {
            k: sorted(str(p.absolute()) for p in v)
            for k, v in hashes.items()
            if len(v) > 1
        }
        print(_pretty_json(duplicates))


if __name__ == '__main__':
    main()
