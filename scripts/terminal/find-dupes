#!/usr/bin/env python3
import argparse
import collections
import hashlib
import json
import pathlib
import re
import shutil
import sys


# lf adds `.~1~` to the end of a file when a copy/move destination exists
LF_DUPLICATE_SUFFIX = '.~1~'
LF_DUPLICATE_PATTERN = re.compile(r'(.+)(\.~\d+\~)+$')

# encounterd with .tar.zst
FIREFOX_DUPLICATE_PATTERN_1 = re.compile(r'(.)(\(\d+\))(\.[^\.]+|\.tar\.[^\.]+)$')
# encounterd this with .exe and .pdf
FIREFOX_DUPLICATE_PATTERN_2 = re.compile(r'(.)(\-\d+)(\.[^\.]+|\.tar\.[^\.]+)$')


def all_files(root):
    stack = [pathlib.Path(root)]
    files = []
    while stack:
        for path in stack.pop().iterdir():
            if path.is_file() and not path.is_symlink():
                files.append(path)
            elif path.is_dir():
                stack.append(path)
    return files


def sha256sum(path):
    hasher = hashlib.sha256()
    try:
        with open(path, 'rb') as handle:
            while data := handle.read(65536):
                hasher.update(data)
    except OSError:
        print(f'error reading "{path}"', file=sys.stderr)
        raise
    return hasher.hexdigest()


def _pretty_json(data):
    def _default_json(obj):
        if isinstance(obj, set):
            return sorted(obj)
        elif isinstance(obj, pathlib.Path):
            return str(obj)
        else:
            raise TypeError
    return json.dumps(
        data,
        indent=2,
        sort_keys=True,
        default=_default_json,
        ensure_ascii=False,
    )


def _sorted_duplicates_key(path):
    path = path.absolute()
    lf_matches = [
        bool(LF_DUPLICATE_PATTERN.match(p))
        for p in path.parts
    ]
    is_lf_dupe = False
    is_firefox_dupe = False

    name = path.name
    while True:
        name, num_subs = LF_DUPLICATE_PATTERN.subn(r'\1', name)
        if num_subs == 0:
            break
        is_lf_dupe = True

    for pat in (FIREFOX_DUPLICATE_PATTERN_1, FIREFOX_DUPLICATE_PATTERN_2):
        new = pat.sub(r'\1\3', name)
        if new != name:
            is_firefox_dupe = True
            name = new
            # only one pattern should be valid
            break

    dupe_score = 0
    if is_lf_dupe:
        dupe_score += 1
    if is_firefox_dupe:
        dupe_score += 1
    return (
        dupe_score,
        lf_matches[-1],  # filename
        any(lf_matches),  # parent dir(s)
        len(lf_matches),  # try to keep least-nested
        str(path),
    )


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--dryrun', action='store_true')
    action_group = parser.add_mutually_exclusive_group()
    action_group.add_argument(
        '-u', '--unique', action='store_true')
    action_group.add_argument(
        '--rm', action='store_true',
        help='remove duplicates (keeps first match)')
    action_group.add_argument(
        '--mv', type=pathlib.Path,
        help='move duplicates to a directory')
    parser.add_argument('path', default=[''], nargs='*',
                        type=lambda x: pathlib.Path(x).resolve().absolute())
    args = parser.parse_args()

    files = set()
    for path in args.path:
        path = pathlib.Path(path).absolute()
        if path.is_dir():
            files.update(all_files(path))
        else:
            files.add(path)

    by_size = collections.defaultdict(set)
    for path in sorted(files):
        by_size[path.stat().st_size].add(path)

    by_hashes = collections.defaultdict(set)
    unique = set()
    for paths in by_size.values():
        if len(paths) == 1:
            unique.update(paths)
        else:
            for path in paths:
                by_hashes[sha256sum(path)].add(path)

    duplicates_by_hash = {}
    for hashval, paths in by_hashes.items():
        if len(paths) == 1:
            unique.update(paths)
        else:
            duplicates_by_hash[hashval] = \
                sorted(paths, key=_sorted_duplicates_key)

    if args.unique:
        print(_pretty_json(unique))
    elif args.rm:
        for paths in duplicates_by_hash.values():
            for path in paths[1:]:
                print(path)
                if not args.dryrun:
                    path.unlink()
    elif args.mv:
        parent = args.mv
        parent.mkdir()
        for hasval, paths in duplicates_by_hash.items():
            hash_parent = parent / hasval
            hash_parent.mkdir()
            for path in paths:
                name = path.name
                while True:
                    dest = hash_parent / name
                    if dest.exists():
                        name += LF_DUPLICATE_SUFFIX
                    else:
                        break
                shutil.move(path, dest)
    else:
        print(_pretty_json(duplicates_by_hash))


if __name__ == '__main__':
    main()
