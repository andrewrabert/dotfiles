#!/usr/bin/env python3
import argparse
import collections
import hashlib
import json
import pathlib
import re
import sys


# lf adds `.~1~` to the end of a file when a copy/move destination exists
LF_DUPLICATE_PATTERN = re.compile(r'.*\.~\d+\~$')


def all_files(root):
    stack = [pathlib.Path(root)]
    files = []
    while stack:
        for path in stack.pop().iterdir():
            if path.is_file() and not path.is_symlink():
                files.append(path)
            elif path.is_dir():
                stack.append(path)
    return files


def sha256sum(path):
    hasher = hashlib.sha256()
    try:
        with open(path, 'rb') as handle:
            while data := handle.read(65536):
                hasher.update(data)
    except OSError:
        print(f'error reading "{path}"', file=sys.stderr)
        raise
    return hasher.hexdigest()


def _pretty_json(data):
    def _default_json(obj):
        if isinstance(obj, set):
            return sorted(obj)
        elif isinstance(obj, pathlib.Path):
            return str(obj)
        else:
            raise TypeError
    return json.dumps(
        data,
        indent=2,
        sort_keys=True,
        default=_default_json,
        ensure_ascii=False,
    )


def _sorted_duplicates_key(path):
    path = path.absolute()
    lf_matches = [
        bool(LF_DUPLICATE_PATTERN.match(p))
        for p in path.parts
    ]
    return (
        lf_matches[-1],  # filename
        any(lf_matches),  # parent dir(s)
        len(lf_matches),  # try to keep least-nested
        str(path),
    )


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-u', '--unique', action='store_true')
    parser.add_argument('--rm', action='store_true',
                        help='remove duplicates (keeps first match)')
    parser.add_argument('path', default=[''], nargs='*',
                        type=lambda x: pathlib.Path(x).resolve().absolute())
    args = parser.parse_args()

    files = set()
    for path in args.path:
        path = pathlib.Path(path).absolute()
        if path.is_dir():
            files.update(all_files(path))
        else:
            files.add(path)

    by_size = collections.defaultdict(set)
    for path in sorted(files):
        by_size[path.stat().st_size].add(path)

    by_hashes = collections.defaultdict(set)
    unique = set()
    for paths in by_size.values():
        if len(paths) == 1:
            unique.update(paths)
        else:
            for path in paths:
                by_hashes[sha256sum(path)].add(path)

    duplicates_by_hash = {}
    for hashval, paths in by_hashes.items():
        if len(paths) == 1:
            unique.update(paths)
        else:
            duplicates_by_hash[hashval] = \
                sorted(paths, key=_sorted_duplicates_key)

    if args.unique:
        print(_pretty_json(unique))
    else:
        if args.rm:
            for paths in duplicates_by_hash.values():
                for path in paths[1:]:
                    print(path)
                    path.unlink()
        else:
            print(_pretty_json(duplicates_by_hash))


if __name__ == '__main__':
    main()
