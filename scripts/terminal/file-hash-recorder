#!/usr/bin/env python3
import argparse
import asyncio
import hashlib
import json
import pathlib
import sys
import tempfile
import zlib

import tqdm


BUFFER_SIZE = 16 * 1024 * 1024


def all_files(root):
    stack = [pathlib.Path(root)]
    files = []
    while stack:
        for path in stack.pop().iterdir():
            if path.is_file():
                files.append(path)
            elif path.is_dir():
                stack.append(path)
    return files


class MultiHash:
    def __init__(self):
        self.checksum_crc32 = 0
        self.hasher_md5 = hashlib.md5()
        self.hasher_sha1 = hashlib.sha1()
        self.hasher_sha256 = hashlib.sha256()

    def update(self, data):
        self.checksum_crc32 = zlib.crc32(data, self.checksum_crc32)
        self.hasher_md5.update(data)
        self.hasher_sha1.update(data)
        self.hasher_sha256.update(data)

    def hashes(self):
        return {
            'crc32': hex(self.checksum_crc32)[2:],
            'md5': self.hasher_md5.hexdigest(),
            'sha1': self.hasher_sha1.hexdigest(),
            'sha256': self.hasher_sha256.hexdigest(),
        }

    @classmethod
    def from_path(cls, path):
        mh = cls()
        with open(path, 'rb') as handle:
            while data := handle.read(BUFFER_SIZE):
                mh.update(data)
        return mh.hashes()


def safe_write(path, data):
    path = pathlib.Path(path)
    if isinstance(data, str):
        data = data.encode()
    with tempfile.NamedTemporaryFile(delete=False, dir=path.parent) as handle:
        temp_path = pathlib.Path(handle.name)
        try:
            temp_path.write_bytes(data)
            temp_path.rename(path)
        finally:
            try:
                temp_path.unlink()
            except FileNotFoundError:
                pass


class ProcessError(Exception):
    def __init__(self, process, message=None):
        self.process = process
        self.message = message

    def __str__(self):
        proc = self.process

        text = f'exit {proc.returncode}'
        if self.message is not None:
            text = f'{text} - {self.message}'

        try:
            args = proc._transport._extra['subprocess'].args
        except (AttributeError, KeyError):
            pass
        else:
            text = f'{text}: {args}'
        return text


async def archive_contents(path):
    proc = await asyncio.create_subprocess_exec(
        'extract', '--list', '--json', '--', path,
        stdout=asyncio.subprocess.PIPE)
    stdout, _ = await proc.communicate()
    if proc.returncode:
        raise ProcessError(proc)

    contents = {}
    for name, info in json.loads(stdout.decode()).items():
        print(path)
        proc = await asyncio.create_subprocess_exec(
            'extract', '--stdout', '--', path, name,
            stdout=asyncio.subprocess.PIPE)
        mh = MultiHash()
        while True:
            if proc.returncode is not None:
                break
            try:
                data = await asyncio.wait_for(
                    proc.stdout.read(BUFFER_SIZE), 1)
                if data:
                    mh.update(data)
                else:
                    break
            except TimeoutError:
                pass
        await proc.wait()
        if proc.returncode:
            raise ProcessError(proc)
        info.update(mh.hashes())
        contents[name] = info
    return contents


def pretty_json_dumps(data):
    return json.dumps(data, indent=2, sort_keys=True, ensure_ascii=False)


async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--no-progress', action='store_true')
    parser.add_argument('--absolute-paths', action='store_true')
    parser.add_argument('-f', '--force', action='store_true')
    parser.add_argument('-o', '--output', type=pathlib.Path)
    parser.add_argument('--archive-contents', action='store_true')
    parser.add_argument('--show-dupes', action='store_true')
    parser.add_argument('--verify', action='store_true')
    parser.add_argument('path', type=pathlib.Path)
    args = parser.parse_args()

    if args.verify:
        if not args.output:
            raise RuntimeError
        results = json.loads(args.output.read_text())

        found = set()
        has_error = False
        for path in tqdm.tqdm(results):
            info = results[path]
            path = pathlib.Path(path)
            if not path.exists():
                has_error = True
                print('File not found:', path, file=sys.stderr)
                continue
            hashes = MultiHash.from_path(path)
            for hash_type, hash_value in hashes.items():
                if hash_value != info[hash_type]:
                    has_error = True
                    print(f'{hash_type} mismatch:', path, file=sys.stderr)
        if has_error:
            sys.exit(1)
        else:
            sys.exit()


    results = {}
    ignore_paths = set()
    output_path = None
    if args.output is not None:
        output_path = args.output
        ignore_paths.add(output_path)
        if output_path.exists():
            results = json.loads(output_path.read_text())

    found = set()

    pending_paths = []
    for path in all_files(args.path):
        if path in ignore_paths:
            continue
        if args.absolute_paths:
            name = str(path.absolute())
        else:
            name = str(path.relative_to(args.path))
        found.add(name)
        stat = path.stat()
        if name in results \
                and results[name]['mtime'] == stat.st_mtime \
                and results[name]['size'] == stat.st_size \
                and not args.force:
            if args.archive_contents:
                if 'archive_contents' in results:
                    continue
            else:
                continue
        pending_paths.append((path, name, stat))

    if not args.no_progress:
        pending_paths = tqdm.tqdm(pending_paths)

    for path, name, stat in pending_paths:
        hashes = MultiHash.from_path(path)
        data = {
            'mtime': stat.st_mtime,
            'size': stat.st_size,
            'sha1': hashes['sha1'],
            'sha256':  hashes['sha256'],
            'md5':  hashes['md5'],
            'crc32':  hashes['crc32'],
        }
        if args.archive_contents \
                and path.name.lower().endswith('.7z'):
            data['archive_contents'] = await archive_contents(path)
        results[name] = data
        if output_path:
            safe_write(
                output_path,
                pretty_json_dumps(results),
            )

    modified = False
    for name in list(results):
        if name not in found:
            results.pop(name)
            modified = True
    if output_path:
        if modified:
            safe_write(
                output_path,
                pretty_json_dumps(results),
            )
        if args.show_dupes:
            by_sha256 = {}
            for path, info in results.items():
                by_sha256.setdefault(info['sha256'], [])
                by_sha256[info['sha256']].append(path)
            dupes_results = {}
            for sha256, items in by_sha256.items():
                if len(items) > 1:
                    dupes_results[sha256] = sorted(items)
            print(pretty_json_dumps(dupes_results))
    else:
        print(pretty_json_dumps(results))


if __name__ == '__main__':
    asyncio.run(main())
