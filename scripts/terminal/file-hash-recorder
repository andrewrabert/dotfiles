#!/usr/bin/env python3
import argparse
import asyncio
import base64
import contextlib
import dataclasses
import hashlib
import json
import pathlib
import sqlite3
import sys
import tempfile
import zlib

import async_executor
import tqdm
import tqdm.asyncio


BUFFER_SIZE = 16 * 1024 * 1024

@dataclasses.dataclass
class FileInfo:
    path: str
    crc32: str
    md5: str
    mtime: float
    sha1: str
    sha256: str
    size: int
    archive_contents: dict = dataclasses.field(default_factory=dict)


@dataclasses.dataclass
class ArchiveContentsInfo:
    parent: str
    path: str
    crc32: str
    md5: str
    mtime: float
    sha1: str
    sha256: str
    size: int


class FileInfoDB:
    SCHEMA = """
    CREATE TABLE IF NOT EXISTS files(
        path,
        crc32,
        md5,
        mtime real,
        sha1,
        sha256,
        size int,

        primary key(path)
    );
    CREATE TABLE IF NOT EXISTS archive_contents(
        parent,
        path,
        crc32,
        md5,
        mtime real,
        sha1,
        sha256,
        size int,

        primary key(parent, path),
        foreign key(parent) references files(path)
    );
    """

    def __init__(self, path):
        self.path = path.absolute()
        self.conn = sqlite3.connect(self.path)
        self.conn.row_factory = self._dict_factory
        cur = self.conn.cursor()
        cur.executescript(self.SCHEMA)
        cur.close()

    def close(self):
        self.conn.close()

    @staticmethod
    def _dict_factory(cursor, row):
        fields = [column[0] for column in cursor.description]
        return {key: value for key, value in zip(fields, row)}

    def upsert_file(self, file_info):
        for ac in file_info.archive_contents.values():
            if ac.parent != file_info.path:
                raise ValueError
        fi = dataclasses.asdict(file_info)
        ac = fi.pop('archive_contents')
        cur = self.conn.cursor()
        cur.execute(
            """
            INSERT INTO files(path, crc32, md5, mtime, sha1, sha256, size)
            VALUES(:path, :crc32, :md5, :mtime, :sha1, :sha256, :size)
            ON CONFLICT DO UPDATE
                SET crc32 = :crc32,
                    md5 = :md5,
                    mtime = :mtime,
                    sha1 = :sha1,
                    sha256 = :sha256,
                    size = :size
            """,
            fi,
        )
        if ac:
            cur.executemany(
                """
                INSERT INTO archive_contents(parent, path, crc32, md5, mtime, sha1, sha256, size)
                VALUES(:parent, :path, :crc32, :md5, :mtime, :sha1, :sha256, :size)
                ON CONFLICT DO UPDATE
                    SET crc32 = :crc32,
                        md5 = :md5,
                        mtime = :mtime,
                        sha1 = :sha1,
                        sha256 = :sha256,
                        size = :size
                """,
                ac.values(),
            )
        self.conn.commit()
        cur.close()

    def delete_file(self, path):
        cur = self.conn.cursor()
        cur.execute(
            'DELETE FROM archive_contents WHERE parent = :parent',
            {'parent': path})
        cur.execute(
            'DELETE FROM files WHERE path = :path',
            {'path': path})
        self.conn.commit()
        cur.close()

    @staticmethod
    def _escape_like(value, escape):
        escape_chars = sorted(
            set(['%', '_', escape]),
            key=lambda x: (0 if x == escape else 1, x))
        for c in escape_chars:
            value = value.replace(c, escape + c)
        return value

    def get_all(self, filepath=None, prefix='', suffix=''):
        data = {}
        cur = self.conn.cursor()

        clause_files = ''
        clause_archive_contents = ''
        params = {}
        if filepath is not None:
            clause_files = 'WHERE path = :filepath'
            clause_archive_contents = 'WHERE parent = :filepath'
            params['filepath'] = filepath
        elif prefix or suffix:
            clause_files = 'WHERE path LIKE :like ESCAPE :escape'
            clause_archive_contents = 'WHERE parent LIKE :like ESCAPE :escape'
            escape = '\\'
            like = '%'.join((
                self._escape_like(prefix, escape),
                self._escape_like(suffix, escape),
            ))
            params['escape'] = escape
            params['like'] = like
            cur.execute('PRAGMA case_sensitive_like = true')

        cur.execute('SELECT * FROM files ' + clause_files, params)
        for finfo in cur.fetchall():
            finfo = FileInfo(**finfo)
            data[finfo.path] = finfo
        cur.close()

        cur = self.conn.cursor()
        cur.execute('SELECT * FROM archive_contents ' + clause_archive_contents, params)
        for acinfo in cur.fetchall():
            acinfo = ArchiveContentsInfo(**acinfo)
            data[acinfo.parent].archive_contents[acinfo.path] = acinfo
        cur.close()
        return data


def all_files(root):
    if root.is_file():
        return [root]
    stack = [pathlib.Path(root)]
    files = []
    while stack:
        for path in stack.pop().iterdir():
            if path.is_file():
                files.append(path)
            elif path.is_dir():
                stack.append(path)
    return files


class MultiHash:
    def __init__(self):
        self.checksum_crc32 = 0
        self.hasher_md5 = hashlib.md5()
        self.hasher_sha1 = hashlib.sha1()
        self.hasher_sha256 = hashlib.sha256()

    def update(self, data):
        self.checksum_crc32 = zlib.crc32(data, self.checksum_crc32)
        self.hasher_md5.update(data)
        self.hasher_sha1.update(data)
        self.hasher_sha256.update(data)

    def hashes(self):
        return {
            'crc32': hex(self.checksum_crc32)[2:],
            'md5': self.hasher_md5.hexdigest(),
            'sha1': self.hasher_sha1.hexdigest(),
            'sha256': self.hasher_sha256.hexdigest(),
        }

    @classmethod
    def from_path(cls, path):
        mh = cls()
        with open(path, 'rb') as handle:
            while data := handle.read(BUFFER_SIZE):
                mh.update(data)
        return mh.hashes()


class ProcessError(Exception):
    def __init__(self, process, message=None):
        self.process = process
        self.message = message

    def __str__(self):
        proc = self.process

        text = f'exit {proc.returncode}'
        if self.message is not None:
            text = f'{text} - {self.message}'

        try:
            args = proc._transport._extra['subprocess'].args
        except (AttributeError, KeyError):
            pass
        else:
            text = f'{text}: {args}'
        return text


async def multi_hash(path):
    proc = await asyncio.create_subprocess_exec(
        'file-hash-recorder', '--multihash', '--', path,
        stdout=asyncio.subprocess.PIPE)
    stdout, _ = await proc.communicate()
    if proc.returncode:
        raise ProcessError(proc)
    return json.loads(stdout)


async def file_hash_recorder(path):
    proc = await asyncio.create_subprocess_exec(
        'file-hash-recorder', '--multihash', '--', '.',
        cwd=path,
        stdout=asyncio.subprocess.PIPE)
    stdout, _ = await proc.communicate()
    if proc.returncode:
        raise ProcessError(proc)
    return json.loads(stdout)


async def archive_contents(path, use_tmpdir=True):
    if use_tmpdir:
        with tempfile.TemporaryDirectory() as tmpdir:
            proc = await asyncio.create_subprocess_exec(
                'extract', '-q', '-p', tmpdir, '--', path)
            await proc.wait()
            if proc.returncode:
                raise ProcessError(proc)

            contents = await file_hash_recorder(tmpdir)
    else:
        proc = await asyncio.create_subprocess_exec(
            'extract', '--list', '--json', '--base64', '--', path,
            stdout=asyncio.subprocess.PIPE)
        stdout, _ = await proc.communicate()
        if proc.returncode:
            raise ProcessError(proc)

        contents = {}
        for name, info in json.loads(stdout.decode()).items():
            name = base64.b64decode(name).decode()
            proc = await asyncio.create_subprocess_exec(
                'extract', '--stdout', '--', path, name,
                stdout=asyncio.subprocess.PIPE)
            mh = MultiHash()
            while True:
                if proc.returncode is not None:
                    break
                try:
                    data = await asyncio.wait_for(
                        proc.stdout.read(BUFFER_SIZE), 1)
                    if data:
                        mh.update(data)
                    else:
                        break
                except TimeoutError:
                    pass
            await proc.wait()
            if proc.returncode:
                raise ProcessError(proc)
            info.update(mh.hashes())
            contents[name] = info
    return contents


def pretty_json_dumps(data):
    return json.dumps(
        data,
        indent=2,
        sort_keys=True,
        ensure_ascii=False)


def json_dumps(data):
    return json.dumps(
        data,
        ensure_ascii=False,
        separators=(',', ':'))


def is_archive(path):
    return path.name.lower().endswith((
        '.7z',
        '.bz2',
        '.gz',
        '.nsz',
        '.rar',
        '.rvz',
        '.tgz',
        '.xz',
        '.zip',
        '.zst',
    ))


def find_default_file_info_db(root_path, must_exist=True):
    if not root_path.exists():
        raise FileNotFoundError

    root_path = root_path.absolute()
    search_dirs = list(root_path.parents)
    if root_path.is_dir():
        search_dirs = [root_path, *search_dirs]

    for path in search_dirs:
        path = path / 'file_info.db'
        if path.is_file():
            return path

    if must_exist:
        raise RuntimeError
    else:
        return root_path / 'file_info.db'


async def _process_path(path,
                        stat=None,
                        hash_archive_contents=True,
                        multihash_subprocess=False,
                        use_tmpdir=False):
    if stat is None:
        stat = path.stat()
    if multihash_subprocess:
        hashes = await multi_hash(path)
    else:
        hashes = MultiHash.from_path(path)
    data = {
        'mtime': stat.st_mtime,
        'size': stat.st_size,
        'sha1': hashes['sha1'],
        'sha256':  hashes['sha256'],
        'md5':  hashes['md5'],
        'crc32':  hashes['crc32'],
    }
    if hash_archive_contents and is_archive(path):
        data['archive_contents'] = await archive_contents(path, use_tmpdir=use_tmpdir)
    return data


async def _process_multihash_root_path(root_path, args, parser):
    results = {}
    for path in all_files(root_path):
        results[path] = await _process_path(path, hash_archive_contents=False)

    if root_path.is_file():
        results = tuple(results.values())[0]
    else:
        results = {
            str(k.relative_to(root_path)): v
            for k, v in results.items()
        }
    print(json_dumps(results))


async def _process_db_root_path(root_path, args, parser, db):
    root_path = root_path.absolute()
    results = {}
    if root_path == db.path.parent:
        results = db.get_all()
    elif db.path.parent in root_path.parents:
        rel_root_path = root_path.relative_to(db.path.parent)
        if root_path.is_dir():
            results = db.get_all(prefix=str(rel_root_path) + '/')
        else:
            results = db.get_all(filepath=str(rel_root_path))

    if args.list or args.show_dupes:
        if args.absolute_paths:
            results = {
                str(db.path.parent) + '/' + k: v
                for k, v in results.items()
            }
        elif root_path != db.path.parent:
            prefix = str(rel_root_path) + '/'
            results = {
                k.removeprefix(prefix): v
                for k, v in results.items()
            }

        results = {
            k: dataclasses.asdict(v)
            for k, v in results.items()
        }
        for info in results.values():
            info.pop('path')
            for ac_info in info['archive_contents'].values():
                ac_info.pop('path')
                ac_info.pop('parent')
            if not info['archive_contents']:
                info.pop('archive_contents')

        if args.show_dupes:
            grouped = {}
            for path, info in results.items():
                key = tuple(
                    info[k]
                    for k in ('size', 'crc32', 'md5', 'sha1', 'sha256')
                )
                grouped.setdefault(key, [])
                grouped[key].append(path)
            dupes_results = [
                sorted(v)
                for v in grouped.values()
                if len(v) > 1
            ]
            print(pretty_json_dumps(dupes_results))
        else:
            print(pretty_json_dumps(results))
        parser.exit()

    if args.verify:
        found = set()
        has_error = False
        print('Verifying', root_path)
        for path in tqdm.tqdm(results):
            info = dataclasses.asdict(results[path])
            path = db.path.parent / path
            if not path.exists():
                has_error = True
                print('File not found:', path, file=sys.stderr)
                continue
            stat = path.stat()
            if stat.st_size != info['size']:
                has_error = True
                print('size mismatch:', path, file=sys.stderr)
            hashes = MultiHash.from_path(path)
            for hash_type, hash_value in hashes.items():
                if hash_value != info[hash_type]:
                    has_error = True
                    print(f'{hash_type} mismatch:', path, file=sys.stderr)
        if has_error:
            sys.exit(1)
        else:
            return

    ignore_paths = {db.path}

    found = set()

    pending_paths = []
    for path in all_files(root_path):
        if path in ignore_paths:
            continue
        if args.absolute_paths:
            name = str(path.absolute())
        else:
            name = str(path.relative_to(db.path.parent))
        found.add(name)
        stat = path.stat()
        if name in results \
                and results[name].mtime == stat.st_mtime \
                and results[name].size == stat.st_size \
                and not args.force:
            if not args.no_archive_contents and is_archive(path):
                if results[name].archive_contents:
                    continue
            else:
                continue
        pending_paths.append((path, name, stat))

    future_info = {}
    executor = async_executor.AsyncExecutor(args.num_procs)
    for path, name, stat in pending_paths:
        future = executor.submit(
            _process_path,
            path,
            stat=stat,
            hash_archive_contents=not args.no_archive_contents,
            multihash_subprocess=args.num_procs > 1,
            use_tmpdir=not args.no_tmpdir,
        )
        future_info[future] = (path, name)

    if not args.no_progress:
        print('Processing', root_path)
        executor = tqdm.asyncio.tqdm(executor)

    try:
        async for future in executor:
            path, name = future_info[future]
            try:
                data = future.result()
            except Exception as e:
                print(f'error processing "{path}": {e}', file=sys.stderr)
                if args.skip_errors:
                    continue
                else:
                    raise


            archive_contents = {
                k: ArchiveContentsInfo(parent=name, path=k, **v)
                for k, v in data.pop('archive_contents', {}).items()
            }
            fileinfo = FileInfo(path=name, archive_contents=archive_contents, **data)
            db.upsert_file(fileinfo)
            results[name] = fileinfo

        for name in results:
            if name not in found:
                db.delete_file(name)
    except KeyboardInterrupt:
        sys.exit(1)


async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--no-progress', action='store_true')
    parser.add_argument('--absolute-paths', action='store_true')

    mode_mutex = parser.add_mutually_exclusive_group()
    mode_mutex.add_argument('-l', '--list', action='store_true')
    mode_mutex.add_argument('--verify', action='store_true')
    mode_mutex.add_argument('--multihash', action='store_true')

    parser.add_argument('-f', '--force', action='store_true')
    parser.add_argument('--mkdb', action='store_true',
                        help='create the database if one does not already exist')
    parser.add_argument('-c', '--compact', action='store_true',
                        help='output compact json')
    parser.add_argument('-o', '--output', type=pathlib.Path)
    parser.add_argument('-n', '--num-procs', type=int, default=1)
    parser.add_argument('--no-archive-contents', action='store_true')
    parser.add_argument('--skip-errors', action='store_true')
    parser.add_argument('--show-dupes', action='store_true')
    parser.add_argument('--no-tmpdir', action='store_true',
                        help='extract to tempdir when hashing archive contents')
    parser.add_argument('path', type=pathlib.Path, nargs='+')
    args = parser.parse_args()

    if len(args.path) > 1:
        if any((args.output, args.show_dupes, args.list, args.multihash)):
            raise RuntimeError

    for path in args.path:
        if args.multihash:
            await _process_multihash_root_path(path, args, parser)
        else:
            output_path = args.output or find_default_file_info_db(
                path, must_exist=not args.mkdb)
            with contextlib.closing(FileInfoDB(output_path)) as db:
                await _process_db_root_path(path, args, parser, db)



if __name__ == '__main__':
    asyncio.run(main())
