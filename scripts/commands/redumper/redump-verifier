#!/usr/bin/env -S uv --quiet run --script
"""
Redump database verification tool with HTTP client support.

# /// script
# dependencies = [
#     "httpx",
# ]
# ///
"""
import argparse
import collections
import asyncio
import contextvars
import hashlib
import io
import json
import logging
import pathlib
import pickle
import re
import sys
import tempfile
import xml.etree.ElementTree
import zipfile

import httpx

ISO_SUFFIXES = (
    ".iso",
    ".mdf",
    ".gcm",  # gamecube
    ".rawiso",  #  generated by discimagecreator+unscrambler for gamecube
)
SUFFIXES = (".bin", *ISO_SUFFIXES)

IGNORE_ENDSWITH = (
    " (Track 0).bin",
    " (Track 00).bin",
    " (Track 1)(-LBA).bin",
    " (Track 01)(-LBA).bin",
    " (Track AA).bin",
    " (Track A).bin",
)

NAME = "redump-renamer"

LOGGER = logging.getLogger(NAME)

_LOG_CONTEXT = {}


def log_context(key, value):
    global _LOG_CONTEXT

    if key not in _LOG_CONTEXT:
        _LOG_CONTEXT[key] = contextvars.ContextVar(key, default=None)

    _LOG_CONTEXT[key].set(value)


class _LogFormatter(logging.Formatter):
    def format(self, record):
        parts = [super().format(record)]

        for key, cvar in _LOG_CONTEXT.items():
            value = cvar.get()
            if value:
                parts.append(f"[{key} {value}]")

        return " ".join(parts)


def setup_logging(name, level=logging.INFO):
    logger = logging.getLogger()
    logger.setLevel(level)

    fmt = _LogFormatter("%(message)s")

    sh = logging.StreamHandler(sys.stdout)
    sh.setLevel(level)
    sh.setFormatter(fmt)
    logger.addHandler(sh)


class Redump:
    _DEFAULT_DAT_PATH = pathlib.Path("/storage/Cache/redump.pickle")

    def __init__(self):
        pathlib.Path(self._DEFAULT_DAT_PATH).parent.mkdir(parents=True, exist_ok=True)

        if self._DEFAULT_DAT_PATH.exists():
            self.hashes = pickle.loads(self._DEFAULT_DAT_PATH.read_bytes())
        else:
            self.hashes = {}

    async def download_datfiles(self):
        """
        Download the redump PC datfile.

        Note: the response does not contain an etag
        """
        LOGGER.info("Updating .dat files")
        systems_dict = {}
        systems = [
            "cdi",  # CD-i
            "dc",  # dreamcast
            "gc",  # gamecube
            "mac",  # macintosh
            "mcd",  # SEGA Mega CD
            "palm",  # palm os
            "pc",  # PC
            "ps2",  # playstation 2
            "ps3",  # playstation 3
            "psp",  # playstation portable
            "psx",  # playstation
            "ss",  # SEGA Saturn
            "wii",  # wii
            "xbox",  # xbox
            "xbox360",  # xbox 360
        ]
        client = httpx.AsyncClient()
        for system in systems:
            LOGGER.debug("Downloading dat for %s", system)
            url = f"http://redump.org/datfile/{system}"
            response = await client.get(url, timeout=15)
            response.raise_for_status()
            archive = zipfile.ZipFile(io.BytesIO(response.content))
            if len(archive.namelist()) != 1:
                raise RuntimeError("expected archive to contain one file")
            dat_file = io.BytesIO()
            dat_file.write(archive.read(archive.namelist()[0]))
            dat_file.seek(0)
            systems_dict[system] = xml.etree.ElementTree.parse(dat_file)

        self.hashes.clear()
        for system, x in systems_dict.items():
            for thing in x.getroot():
                if thing.tag != "game":
                    continue
                hashes_by_name = {}
                for item in thing:
                    if item.tag != "rom":
                        continue
                    if not item.attrib["name"].lower().endswith(SUFFIXES):
                        continue
                    hashes_by_name[item.attrib["name"]] = item.attrib["sha1"]
                thing_hashes = tuple(
                    hashes_by_name[name] for name in sorted(hashes_by_name)
                )
                self.hashes[thing_hashes] = system, thing.attrib["name"]
        self._DEFAULT_DAT_PATH.write_bytes(pickle.dumps(self.hashes))


async def find_dump_hashes(root, recorded=None):
    if recorded:
        files = []
        for p in recorded["archive_contents"]:
            if "/" in recorded:
                raise ValueError
            files.append(pathlib.Path(p))
    else:
        iterator = root.iterdir() if root.is_dir() else [root]

        files = [path for path in iterator if path.is_file()]
        hash_cache = None
        if len(files) == 1 and files[0].name == "dump.tar.zst":
            hash_cache = FILE_HASH_RECORDER[str(files[0])]["archive_contents"]
            files = [pathlib.Path(p) for p in hash_cache if "/" not in p]

    paths = [
        path
        for path in files
        if path.name.endswith(SUFFIXES) and not path.name.endswith(IGNORE_ENDSWITH)
    ]

    if len(paths) > 1:
        iso_paths = [path for path in paths if path.name.endswith(".iso")]
        if len(iso_paths) == 1:
            paths = iso_paths
        else:
            for path in paths:
                if not path.name.endswith(".bin"):
                    raise RuntimeError(f"unexpected files {root}")

    if recorded:
        hashes = {
            path: recorded["archive_contents"][str(path)]["sha1"]
            for path in sorted(paths)
        }
    else:
        if hash_cache:
            hashes = {path: hash_cache[str(path)]["sha1"] for path in sorted(paths)}
        else:
            hashes = {path: sha1sum(path) for path in sorted(paths)}
    LOGGER.debug("sha1 hashes %s", hashes)
    return hashes


FILE_HASH_RECORDER = {}


def sha1sum(path):
    if FILE_HASH_RECORDER:
        return FILE_HASH_RECORDER[str(path)]["sha1"]

    hasher = hashlib.sha1()
    with open(path, "rb") as handle:
        while data := handle.read(65536):
            hasher.update(data)
    return hasher.hexdigest()


FILE_HASH_RECORDER_LIST = {}


async def file_hash_recorder_list(path):
    path = str(path.absolute())
    result = FILE_HASH_RECORDER_LIST.get(path)
    if not result:
        proc = await asyncio.create_subprocess_exec(
            "file-hash-recorder",
            "--absolute-paths",
            "--list",
            "--",
            path,
            stderr=asyncio.subprocess.DEVNULL,
            stdout=asyncio.subprocess.PIPE,
        )
        stdout, _ = await proc.communicate()
        if proc.returncode:
            return None
            raise RuntimeError(proc.returncode)
        result = json.loads(stdout)
        FILE_HASH_RECORDER_LIST.update(result)
    return FILE_HASH_RECORDER_LIST.get(path)


async def extract(path, dest_dir):
    proc = await asyncio.create_subprocess_exec(
        "extract", "--quiet", "-p", str(dest_dir), "--", str(path)
    )
    await proc.wait()
    if proc.returncode:
        raise RuntimeError(proc.returncode)


async def get_name(path, redump, recorded=None):
    arg_hashes = await find_dump_hashes(path, recorded=recorded)
    try:
        system, name = redump.hashes[tuple(arg_hashes.values())]
    except KeyError:
        return None, path.name
    return system, name


REDUMP_ID_PATH_PATTERN = re.compile(r"^redump_(?P<redump_id>\d+) ")


def all_directories(root):
    stack = [root]
    dirs = [root]
    while stack:
        for path in stack.pop().iterdir():
            if path.is_dir():
                dirs.append(path)
                stack.append(path)
    return dirs


async def process_path(redump, path, args):
    log_context("path", path)

    file_paths = []
    has_bin = False
    has_cue = False
    has_iso = False

    is_archive = False
    if path.is_file():
        if path.suffix in ISO_SUFFIXES:
            suffix = ".iso"
            system, name = await get_name(path, redump)
            file_paths.append(path)
            has_iso = True
        elif path.suffix.lower() in (".7z", ".rvz"):
            suffix = path.suffix.lower()
            recorded = await file_hash_recorder_list(path)
            stat = path.stat()
            if stat.st_size != recorded["size"]:
                raise ValueError
            if stat.st_mtime != recorded["mtime"]:
                raise ValueError
            system, name = await get_name(path, redump, recorded=recorded)
            is_archive = True
        elif path.suffixes[-2:] == [".tar", ".zst"]:
            suffix = ".tar.zst"
            parent = pathlib.Path("~").expanduser()
            with tempfile.TemporaryDirectory(dir=parent) as tmp:
                dest = pathlib.Path(tmp) / path.name
                await extract(path, dest)
                system, name = await get_name(dest, redump)
        else:
            raise RuntimeError()
    else:
        suffix = ""
        system, name = await get_name(path, redump)

        for p in path.iterdir():
            if p.is_file():
                match p.suffix.lower():
                    case s if s in ISO_SUFFIXES:
                        has_iso = True
                    case ".bin":
                        has_bin = True
                    case ".cue":
                        has_cue = True
    if is_archive or has_iso or (has_cue and has_bin):
        if name and system:
            name = f"{name}{suffix}"
            if not args.no_prefix:
                name = f"redump_{system} {name}"
            target = path.with_name(name)

            if path == target:
                LOGGER.info("✅ validated")
                return

            if args.rename:
                LOGGER.info("ℹ️ RENAMING %s", target)
                if target.exists():
                    raise RuntimeError(f"target exists {target}")
                path.rename(target)
            else:
                LOGGER.info("ℹ️ REDUMP ID CORRECT, BUT NAME DIFFERS %s", target)
        else:
            LOGGER.error("⚠️ no match")
            if not path.name.startswith("unknown "):
                target = path.absolute().with_name(f"unknown {path.absolute().name}")
                while target.exists():
                    target = target.with_name(f"{target.name}_")
                if args.rename:
                    path.rename(target)


async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--hashes",
        type=pathlib.Path,
        help="use hashes from a file generated by file-hash-recorder",
    )
    parser.add_argument("--rename", action="store_true")
    parser.add_argument("--update", action="store_true")
    parser.add_argument("--verbose", action="store_true")
    parser.add_argument("--no-prefix", action="store_true")
    parser.add_argument("path", type=pathlib.Path, nargs="*")
    args = parser.parse_args()

    paths = {path.absolute() for path in args.path}
    if paths:
        common = None
        parent_counts = collections.Counter()
        for path in paths:
            path = path.absolute()
            for parent in path.parents:
                parent_counts[parent] += 1
        if path.is_dir():
            parent_counts[path] += 1

        for k, v in parent_counts.items():
            if v == len(paths):
                if common is None:
                    common = k
                elif len(k.parts) > len(common.parts):
                    common = k
        await file_hash_recorder_list(common)

    if not args.update and not paths:
        parser.error("a path is required")

    setup_logging(NAME, level=logging.DEBUG if args.verbose else logging.INFO)

    redump = Redump()

    if args.update or not redump.hashes:
        await redump.download_datfiles()

    if args.update and not paths:
        parser.exit()

    if args.hashes:
        FILE_HASH_RECORDER.update(json.loads(args.hashes.read_text()))

    dirs = set()
    for path in paths:
        if path.is_dir():
            dirs.update(all_directories(path))
        else:
            dirs.add(path)

    for path in dirs:
        try:
            await process_path(redump, path, args)
        except Exception as e:
            LOGGER.error("💣error processing %s (%s)", path, e)


if __name__ == "__main__":
    asyncio.run(main())
