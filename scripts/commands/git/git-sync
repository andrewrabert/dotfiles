#!/usr/bin/env python3
import argparse
import enum
import os
import pathlib
import shutil
import subprocess
import sys
import urllib.parse

CODE_ROOT_DIR = pathlib.Path.home() / "src"
CACHE_ROOT_DIR = (
    pathlib.Path(os.environ["XDG_CACHE_HOME"])
    if os.environ.get("XDG_CACHE_HOME")
    else pathlib.Path.home() / ".cache"
)
CACHE_PATH = CACHE_ROOT_DIR / "git-sync"
BACKUP_CACHE_PATH = CACHE_ROOT_DIR / "git-sync-backup"


class Git:
    @staticmethod
    def clone(repository, directory):
        subprocess.run(["git", "clone", repository, directory], check=True)

    @staticmethod
    def fetch(directory):
        subprocess.run(
            ["git", "-C", str(directory), "fetch", "--all"], check=True
        )


def _find_git_repos_old(root):
    stack = [root]
    while stack:
        for path in stack.pop().iterdir():
            if path.is_dir():
                if path.name == ".git":
                    yield path.parent.absolute()
                else:
                    stack.append(path)


def _find_git_repos(root):
    all_repos = set()
    proc = subprocess.run(
        [
            "fd",
            "--unrestricted",
            "--type=d",
            "--print0",
            r"^\.git$",
            "--",
            root,
        ],
        stdout=subprocess.PIPE,
        check=True,
    )
    for item in proc.stdout.split(b"\0"):
        if not item:
            continue
        path = pathlib.Path(item.decode())
        repo = path.parent.relative_to(root)
        all_repos.add(repo)
    repos = []
    for repo in all_repos:
        is_subrepo = False
        for p in repo.parents:
            if p in all_repos:
                is_subrepo = True
                break
        if is_subrepo:
            continue
        repos.append(repo)
    return repos


class SrcName(enum.StrEnum):
    ARCH_LINUX_AUR = "aur"
    CODEBERG = "codeberg"
    GITHUB = "github"
    GITLAB = "gitlab"
    KDE_INVENT = "invent.kde.org"
    NULLSUM_NET = "git.nullsum.net"


def clean_empty_dirs(root, dry_run=False):
    """Recursively delete empty directories, skipping git repos."""
    deleted_count = 0

    def is_git_repo(path):
        return (path / ".git").exists()

    def clean_dir(path):
        nonlocal deleted_count

        if not path.is_dir():
            return False

        # Skip if this is a git repo
        if is_git_repo(path):
            return False

        # Don't delete the root directory itself
        is_root = path == root

        # Check if directory contains any files (not just directories)
        if not is_root:
            try:
                has_files = any(not item.is_dir() for item in path.iterdir())
                if has_files:
                    return False
            except PermissionError:
                return False

        # Process subdirectories first
        try:
            for item in path.iterdir():
                if item.is_dir():
                    clean_dir(item)
        except PermissionError:
            return False

        # Check if directory is now empty and delete it (but not root)
        if not is_root:
            try:
                contents = list(path.iterdir())
                if not contents:
                    print(f"Deleting {path}")
                    if not dry_run:
                        path.rmdir()
                    deleted_count += 1
                    return True
            except (PermissionError, OSError):
                pass

        return False

    clean_dir(root)
    return deleted_count


def parse_repo(repo):
    repo = repo.lower()
    if ":" not in repo:
        parts = repo.split("/")
        if len(parts) < 3:
            raise ValueError("invalid repo")
        host = parts[0]
        path = "/".join(parts[1:])
    else:
        parsed = urllib.parse.urlparse(repo)
        path = parsed.path.strip("/")
        if parsed.scheme in ("http", "https"):
            # TODO: parse .ssh/config for HostName
            match parsed.netloc:
                case "codeberg.org":
                    host = SrcName.CODEBERG
                    parts = path.split("/")
                    if len(parts) < 2:
                        raise RuntimeError("invalid codeberg url")
                    path = "/".join(parts[0:])
                case "github.com" | "www.github.com":
                    host = SrcName.GITHUB
                    parts = path.split("/")
                    if len(parts) < 2:
                        raise RuntimeError("invalid github url")
                    path = "/".join(parts[0:2])
                case "gitlab.com" | "www.gitlab.com":
                    host = SrcName.GITLAB
                    parts = path.split("/")
                    if len(parts) < 2:
                        raise RuntimeError("invalid gitlab url")
                    path = "/".join(parts[0:])
                case "aur.archlinux.org":
                    host = SrcName.ARCH_LINUX_AUR
                    parts = path.split("/")
                    if parts[0] == "packages":
                        path = parts[1]
                case "git.nullsum.net":
                    host = SrcName.NULLSUM_NET
                    parts = path.split("/")
                    if len(parts) < 2:
                        raise RuntimeError("invalid git.nullsum.net url")
                    path = "/".join(parts)
                case "invent.kde.org" | "www.gitlab.com":
                    host = SrcName.KDE_INVENT
                    parts = path.split("/")
                    if len(parts) < 2:
                        raise RuntimeError("invalid invent.kde.org url")
                    path = "/".join(parts[0:])
                case _:
                    raise NotImplementedError(
                        f"unhandled netloc: {parsed.netloc}"
                    )
        elif parsed.scheme:
            host = parsed.scheme
        else:
            raise ValueError("missing git host")

    repo_path = path
    repo_dir = CODE_ROOT_DIR / host.lower() / repo_path.lower()
    return repo_dir, host, path


def main():
    parser = argparse.ArgumentParser("git-sync")
    cache_group = parser.add_mutually_exclusive_group()
    cache_group.add_argument("--show-cache", action="store_true")
    cache_group.add_argument("--show-backup-cache", action="store_true")
    parser.add_argument("--build-cache", action="store_true")
    parser.add_argument("--clean-empty", action="store_true")
    parser.add_argument("--dry-run", action="store_true")
    parser.add_argument("--parse-url", action="store_true")
    parser.add_argument("--rm", action="store_true")
    parser.add_argument("repo", nargs="?")
    args = parser.parse_args()

    # Handle --parse-url (no CODE_ROOT_DIR check needed)
    if args.parse_url:
        if not args.repo:
            print(
                "error: --parse-url requires a URL argument", file=sys.stderr
            )
            parser.exit(1)
        try:
            _, host, path = parse_repo(args.repo)
            print(f"{host}:{path}")
        except Exception as e:
            print(f"error: {e}", file=sys.stderr)
            parser.exit(1)
        return

    if not CODE_ROOT_DIR.is_dir():
        return

    CACHE_PATH.touch()
    BACKUP_CACHE_PATH.touch()

    cache = []
    cache_modified = False
    for repo in CACHE_PATH.read_text().splitlines():
        repo_dir = CODE_ROOT_DIR / repo
        if not repo_dir.exists():
            try:
                repo_dir, _, _ = parse_repo(repo)
            except Exception:
                continue
        if repo_dir.is_dir():
            cache.append(repo)
        else:
            cache_modified = True

    backup_cache = []
    backup_cache_modified = False
    for repo in BACKUP_CACHE_PATH.read_text().splitlines():
        repo_dir = CODE_ROOT_DIR / repo
        if not repo_dir.exists():
            try:
                repo_dir, _, _ = parse_repo(repo)
            except Exception:
                continue
        if repo_dir.is_dir():
            backup_cache.append(repo)
        else:
            backup_cache_modified = True

    if args.show_cache:
        print("\n".join(cache))
    elif args.show_backup_cache:
        print("\n".join(backup_cache))
    elif args.clean_empty:
        deleted = clean_empty_dirs(CODE_ROOT_DIR, dry_run=args.dry_run)
        print(f"Deleted {deleted} empty directories")
    elif args.build_cache:
        # Load existing cache order
        existing_cache = []
        existing_cache_set = set()
        for repo in CACHE_PATH.read_text().splitlines():
            if repo:  # skip empty lines
                existing_cache.append(repo)
                existing_cache_set.add(repo)

        existing_backup_cache = []
        existing_backup_cache_set = set()
        for repo in BACKUP_CACHE_PATH.read_text().splitlines():
            if repo:  # skip empty lines
                existing_backup_cache.append(repo)
                existing_backup_cache_set.add(repo)

        # Find all current repos
        regular_repos = []
        backup_repos = []
        for repo in _find_git_repos(CODE_ROOT_DIR):
            try:
                host = SrcName(repo.parts[0])
            except ValueError:
                host = None
            if host is None:
                repo_str = str(repo)
            else:
                repo_str = f"{host}:{'/'.join(repo.parts[1:])}"

            # Check if repo is in backup/ directory
            if str(repo).startswith("backup/"):
                backup_repos.append(repo_str)
            else:
                regular_repos.append(repo_str)

        # Separate regular repos into existing (in order) and new (to be sorted)
        regular_repos_set = set(regular_repos)
        ordered_regular = [r for r in existing_cache if r in regular_repos_set]
        new_regular = sorted(
            [r for r in regular_repos if r not in existing_cache_set]
        )

        # Separate backup repos into existing (in order) and new (to be sorted)
        backup_repos_set = set(backup_repos)
        ordered_backup = [
            r for r in existing_backup_cache if r in backup_repos_set
        ]
        new_backup = sorted(
            [r for r in backup_repos if r not in existing_backup_cache_set]
        )

        # Combine: existing order + new sorted
        final_regular_repos = ordered_regular + new_regular
        final_backup_repos = ordered_backup + new_backup

        CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)
        CACHE_PATH.write_text("\n".join(final_regular_repos))
        BACKUP_CACHE_PATH.write_text("\n".join(final_backup_repos))
        print(
            f"Built cache with {len(final_regular_repos)} regular repositories"
        )
        print(
            f"Built backup cache with {len(final_backup_repos)} backup repositories"
        )
    elif args.repo is None:
        for repo_dir in _find_git_repos(pathlib.Path().absolute()):
            print(repo_dir)
            Git.fetch(repo_dir)
    else:
        repo_dir = CODE_ROOT_DIR / args.repo
        if repo_dir.exists():
            repo = args.repo
        else:
            repo = args.repo.lower()
            repo_dir, host, path = parse_repo(repo)

        # Determine if this is a backup repo
        is_backup = args.repo.startswith("backup/")

        if args.rm:
            if repo_dir.exists():
                shutil.rmtree(repo_dir)
                parser.exit()
            if is_backup:
                if repo in backup_cache:
                    backup_cache_modified = True
                    backup_cache.remove(repo)
            else:
                if repo in cache:
                    cache_modified = True
                    cache.remove(repo)
        else:
            if not repo_dir.exists():
                if host == SrcName.KDE_INVENT:
                    repo = f"https://{host}/{path}"
                else:
                    repo = f"{host}:{path}"
                Git.clone(repo, repo_dir)

            if is_backup:
                backup_cache_modified = True
                if repo in backup_cache:
                    backup_cache.remove(repo)
                backup_cache.append(repo)
            else:
                cache_modified = True
                if repo in cache:
                    cache.remove(repo)
                cache.append(repo)
            print(repo_dir)

    if cache_modified:
        CACHE_PATH.write_text("\n".join(cache))
    if backup_cache_modified:
        BACKUP_CACHE_PATH.write_text("\n".join(backup_cache))


if __name__ == "__main__":
    main()
