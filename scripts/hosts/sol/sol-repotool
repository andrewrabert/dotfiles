#!/usr/bin/env -S uv run --script
"""
# /// script
# dependencies = [
#     "pyalpm",
#     "tqdm",
# ]
# ///
"""

import argparse
import asyncio
import dataclasses
import fcntl
import json
import logging
import os
import pathlib
import re
import shutil
import subprocess
import sys

import pyalpm
import tqdm

# Use system PATH for subcommands (exclude uv/venv directories)
os.environ["PATH"] = ":".join(
    p
    for p in os.environ.get("PATH", "").split(":")
    if ".cache/uv" not in p and "/.venv/" not in p
)

LOCK_FILE = (
    pathlib.Path(os.environ.get("XDG_RUNTIME_DIR", "/tmp"))
    / "sol-repotool.lock"
)

# Set up logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")


@dataclasses.dataclass
class DependencyInfo:
    """Parsed dependency information from .SRCINFO"""

    depends: list[str]
    optdepends: list[str]
    makedepends: list[str]


@dataclasses.dataclass
class PackageUpdateInfo:
    """Information about a package update"""

    name: str
    current_version: str
    new_version: str
    has_update: bool
    is_vcs: bool
    deps: (
        DependencyInfo | None
    )  # None if no update, otherwise new deps from .SRCINFO


DATABASE = "sol"

# Pre-compile regex for better performance when parsing many dependencies
VERSION_CONSTRAINT_RE = re.compile(r"[<>=]")


def strip_version(dep_string):
    """Strip version information from a dependency string.

    Examples:
        "munt>=2.7.2" -> "munt"
        "foo=1.0" -> "foo"
        "bar: optional description" -> "bar"
    """
    # Handle optional dependency format with description
    dep_string = str(dep_string).split(":")[0].strip()
    # Strip version constraints
    return VERSION_CONSTRAINT_RE.split(dep_string)[0]


async def run_cmd(*cmd, check=False, capture_output=False, **kwargs):
    """
    Run a command using asyncio subprocess.

    Args:
        *cmd: Command and arguments
        check: If True, raise CalledProcessError on non-zero exit
        capture_output: If True, capture stdout/stderr
        **kwargs: Additional arguments for create_subprocess_exec

    Returns:
        CompletedProcess-like object with returncode, stdout, stderr
    """
    if capture_output:
        kwargs.setdefault("stdout", asyncio.subprocess.PIPE)
        kwargs.setdefault("stderr", asyncio.subprocess.PIPE)

    proc = await asyncio.create_subprocess_exec(*cmd, **kwargs)
    stdout, stderr = await proc.communicate()

    if check and proc.returncode != 0:
        raise subprocess.CalledProcessError(
            proc.returncode, cmd, output=stdout, stderr=stderr
        )

    # Return a simple object with the same interface as subprocess.CompletedProcess
    class Result:
        def __init__(self, returncode, stdout, stderr):
            self.returncode = returncode
            self.stdout = stdout
            self.stderr = stderr

    return Result(proc.returncode, stdout, stderr)


class AURCLI:
    """Static methods for aurutils CLI commands."""

    @staticmethod
    async def _run(*args, cwd=None, capture_output=True):
        """Run an aur command and return (returncode, stdout, stderr)."""
        kwargs = {}
        if cwd:
            kwargs["cwd"] = str(cwd)
        if capture_output:
            kwargs["stdout"] = asyncio.subprocess.PIPE
            kwargs["stderr"] = asyncio.subprocess.PIPE

        proc = await asyncio.create_subprocess_exec("aur", *args, **kwargs)
        stdout, stderr = await proc.communicate()
        return proc.returncode, stdout, stderr

    @staticmethod
    async def fetch(pkgbase: str, cwd: pathlib.Path, reset: bool = False):
        """Fetch a package from AUR. Returns True on success."""
        args = ["fetch"]
        if reset:
            args.append("--reset")
        args.extend(["--", pkgbase])
        returncode, _, _ = await AURCLI._run(*args, cwd=cwd)
        return returncode == 0

    @staticmethod
    async def sync(
        packages: list[str],
        database: str,
        cwd: pathlib.Path = None,
        makepkg_args: list[str] = None,
        no_ver: bool = False,
    ):
        """Sync packages from AUR. Returns True on success."""
        args = [
            "sync",
            "--no-view",
            "--no-confirm",
            "--sign",
            f"--database={database}",
        ]
        if no_ver:
            args.append("--no-ver")
        if makepkg_args:
            args.extend(["--margs", *makepkg_args])
        args.extend(["--", *packages])
        returncode, _, _ = await AURCLI._run(
            *args, cwd=cwd, capture_output=False
        )
        return returncode == 0

    @staticmethod
    async def build(
        database: str,
        cwd: pathlib.Path,
        makepkg_args: list[str] = None,
        force: bool = False,
    ):
        """Build a package in the given directory. Returns True on success."""
        args = ["build", f"--database={database}", "--sign"]
        if force:
            args.append("--force")
        if makepkg_args:
            args.extend(["--margs", *makepkg_args])
        returncode, _, _ = await AURCLI._run(
            *args, cwd=cwd, capture_output=False
        )
        return returncode == 0

    @staticmethod
    async def depends(package: str, optdeps: bool = False):
        """Get dependencies for a package. Returns (required, optional) sets."""
        args = ["depends"]
        if optdeps:
            args.append("--optdeps")
        args.extend(["--", package])
        returncode, stdout, _ = await AURCLI._run(*args)

        deps = set()
        if returncode == 0 and stdout:
            for line in stdout.decode().splitlines():
                parts = line.split()
                if len(parts) >= 2:
                    deps.add(parts[1])
        return deps


@dataclasses.dataclass
class Package:
    name: str
    version: str
    depends: list
    optdepends: list
    makedepends: list
    provides: list
    pkgbase: str

    def __post_init__(self):
        """Strip version information from dependency fields."""
        self.depends = [strip_version(d) for d in self.depends]
        self.optdepends = [strip_version(d) for d in self.optdepends]
        self.makedepends = [strip_version(d) for d in self.makedepends]
        self.provides = [strip_version(p) for p in self.provides]

    @classmethod
    def from_alpm(cls, alpm_pkg):
        """Create a Package instance from an alpm.Package object.

        Args:
            alpm_pkg: A pyalpm.Package object

        Returns:
            Package instance with dependency versions stripped
        """
        return cls(
            name=alpm_pkg.name,
            version=alpm_pkg.version,
            depends=alpm_pkg.depends,
            optdepends=alpm_pkg.optdepends,
            makedepends=alpm_pkg.makedepends,
            provides=alpm_pkg.provides,
            pkgbase=alpm_pkg.base,
        )


class PackageDatabase:
    """Helper class to manage pyalpm database access."""

    OFFICIAL_REPOS = [
        "core",
        "extra",
        "multilib",
    ]

    def __init__(self, repo_name):
        self.repo_name = repo_name
        self.handle = pyalpm.Handle("/", "/var/lib/pacman")
        self._sol_db = self.handle.register_syncdb(self.repo_name, 0)

    @property
    def sol_db(self):
        """Get the sol repository database."""
        return self._sol_db

    def get_official_dbs(self):
        """Get official Arch repository databases (core, extra, multilib)."""
        for repo in self.OFFICIAL_REPOS:
            try:
                self.handle.register_syncdb(repo, 0)
            except Exception:
                pass  # Already registered
        return self.handle.get_syncdbs()

    def get_all_packages(self):
        """Get all packages in the sol repository."""
        packages = []
        for pkg in self.sol_db.pkgcache:
            packages.append(Package.from_alpm(pkg))
        return packages

    def get_package(self, name):
        """Get a specific package by name."""
        return self.sol_db.get_pkg(name)

    def load_package_file(self, file_path):
        """Load a package from a file."""
        return self.handle.load_pkg(str(file_path))


DOTFILES_PRIVATE = pathlib.Path(os.environ["DOTFILES_PRIVATE"])
CONFIG_PATH = DOTFILES_PRIVATE / "sol-repotool.json"


class ConfigFile:
    def __init__(self, path):
        self.path = pathlib.Path(path)
        self._load()

    def _load(self):
        self.aur = set()
        self.manual = set()
        if self.path.exists():
            data = json.loads(self.path.read_text())
            packages = data.get("packages", {})
            self.aur.update(packages.get("aur", []))
            self.manual.update(packages.get("manual", []))

    def _dump(self):
        self.path.parent.mkdir(parents=True, exist_ok=True)
        data = {
            "packages": {
                "aur": sorted(self.aur),
                "manual": sorted(self.manual),
            }
        }
        self.path.write_text(json.dumps(data, indent=2, sort_keys=True))

    def get_explicit_packages(self):
        return set(self.aur)

    def get_no_update_packages(self):
        return set(self.manual)

    def add_packages(self, *packages):
        updated = self.aur.copy()
        updated.update(packages)
        if updated != self.aur:
            self.aur = updated
            self._dump()

    def remove_packages(self, *packages):
        updated = self.aur - set(packages)
        if updated != self.aur:
            self.aur = updated
            self._dump()

    def add_no_update(self, *packages):
        updated = self.manual.copy()
        updated.update(packages)
        if updated != self.manual:
            self.manual = updated
            self._dump()

    def remove_no_update(self, *packages):
        updated = self.manual - set(packages)
        if updated != self.manual:
            self.manual = updated
            self._dump()


class Database:
    DIRECTORY_BASE = pathlib.Path("/var/cache/pacman")
    AURUTILS_CACHE = pathlib.Path.home() / ".cache/aurutils/sync"

    def __init__(
        self,
        name,
        /,
        aurutils_cache=None,
        directory=None,
    ):
        self.name = name

        self.directory = self.DIRECTORY_BASE / name
        if directory:
            self.directory = pathlib.Path(directory)

        self.aurutils_cache = self.AURUTILS_CACHE
        if aurutils_cache:
            self.aurutils_cache = pathlib.Path(aurutils_cache)

        self.config = ConfigFile(CONFIG_PATH)

        self.db_tar = self.directory / f"{self.name}.db.tar"
        self.db_tar_link = self.directory / f"{self.name}.db"
        self.files_tar = self.directory / f"{self.name}.files.tar"
        self.db_files_link = self.directory / f"{self.name}.files"

        self.pkg_db = PackageDatabase(self.name)

    async def _sign_database(self):
        tar_link = self.directory / f"{self.name}.db"
        files_link = self.directory / f"{self.name}.files"

        for path in [tar_link, files_link]:
            dest = path.with_name(path.name + ".sig")
            await run_cmd(
                "gpg",
                "--yes",
                "--detach-sign",
                "--use-agent",
                "--no-armor",
                "--output",
                str(dest),
                str(path),
                check=True,
            )

    async def cleanup(self):
        for path in self.aurutils_cache.iterdir():
            for subpath in (path / "src", path / "pkg"):
                if not subpath.exists():
                    continue
                await run_cmd(
                    "chmod", "-R", "777", "--", str(subpath), check=True
                )
                try:
                    shutil.rmtree(subpath)
                except FileNotFoundError:
                    pass
                except OSError as e:
                    logger.warning(f"Failed to remove {subpath}: {e}")

    async def _get_dependencies(self, package_name, pkg_db=None):
        """
        Get all dependencies for a package.
        Returns a dict with 'required' and 'optional' lists.
        """
        # First try pyalpm if pkg_db is provided (much faster)
        if pkg_db:
            pkg = pkg_db.get_package(package_name)
            if pkg:
                required = []
                for dep in pkg.depends:
                    dep_name = strip_version(dep)
                    if dep_name and dep_name != package_name:
                        required.append(dep_name)

                optional = []
                for optdep in pkg.optdepends:
                    dep_name = strip_version(optdep)
                    if dep_name and dep_name != package_name:
                        optional.append(dep_name)

                return {
                    "required": sorted(required),
                    "optional": sorted(optional),
                }

        # Fallback to aur depends for packages not in sol database
        required_deps = await AURCLI.depends(package_name)
        required_deps.discard(package_name)

        all_deps = await AURCLI.depends(package_name, optdeps=True)
        all_deps.discard(package_name)

        # Optional deps are those in all_deps but not in required_deps
        optional_deps = all_deps - required_deps

        return {
            "required": sorted(list(required_deps)),
            "optional": sorted(list(optional_deps)),
        }

    async def _build_dependency_tree(self, packages, verbose=False):
        """
        Build a dependency tree for the given packages.
        Returns a dict mapping each package to its required and optional dependencies.
        Format: {"package_name": {"required": [...], "optional": [...]}}
        """
        # Initialize pyalpm to speed up dependency lookups
        dep_tree = {}

        for i, pkg in enumerate(packages, 1):
            if verbose:
                print(f"[{i}/{len(packages)}] Checking package: {pkg}")

            deps = await self._get_dependencies(pkg, pkg_db=self.pkg_db)

            if verbose:
                req_count = len(deps["required"])
                opt_count = len(deps["optional"])
                print(
                    f"  Found {req_count} required, {opt_count} optional dependencies"
                )

            dep_tree[pkg] = deps

        return dep_tree

    def package_names_from_json(self):
        """Get all package names from JSON file (packages + no_update)."""
        all_packages = self.config.get_explicit_packages()
        all_packages.update(self.config.get_no_update_packages())
        return list(all_packages)

    def packages(self):
        """Get all packages in the repository using pyalpm."""
        return self.pkg_db.get_all_packages()

    def _calculate_needed_packages(self):
        """
        Get all dependencies (depends, optdepends, makedepends) for a set of packages.
        Returns a set of package names including the input packages and all their deps.
        """
        all_packages = set(self.config.get_explicit_packages())

        # Process packages until we've expanded all dependencies
        to_process = all_packages.copy()
        processed = set()

        while to_process:
            pkg_name = to_process.pop()
            if pkg_name in processed:
                continue
            processed.add(pkg_name)

            pkg = self.pkg_db.get_package(pkg_name)
            if not pkg:
                continue

            # Extract and add all dependency types
            for dep in pkg.depends:
                dep_name = strip_version(dep)
                if dep_name and dep_name not in all_packages:
                    all_packages.add(dep_name)
                    to_process.add(dep_name)

            for optdep in pkg.optdepends:
                dep_name = strip_version(optdep)
                if dep_name and dep_name not in all_packages:
                    all_packages.add(dep_name)
                    to_process.add(dep_name)

            for makedep in pkg.makedepends:
                dep_name = strip_version(makedep)
                if dep_name and dep_name not in all_packages:
                    all_packages.add(dep_name)
                    to_process.add(dep_name)

        return all_packages

    def get_orphaned_packages(self):
        """
        Business Logic: Find packages that should be removed from the repository.

        Orphans = packages currently in repo - packages we need to keep

        A package is kept if:
        - Its name is needed, OR
        - Its pkgbase is needed, OR
        - It provides something that's needed
        - It's in the manual set (never auto-remove)
        """
        in_repo = {pkg.name for pkg in self.packages()}
        to_keep = self._calculate_needed_packages()
        manual = self.config.get_no_update_packages()
        orphans = set()

        for name in in_repo - to_keep:
            pkg = Package.from_alpm(self.pkg_db.get_package(name))

            # Check if package is needed by name, pkgbase, or any provides
            identifiers = [pkg.name, pkg.pkgbase, *pkg.provides]
            is_needed = any(
                identifier in to_keep for identifier in identifiers
            )

            # Manual packages are never auto-removed
            is_manual = pkg.name in manual or pkg.pkgbase in manual

            if not is_needed and not is_manual:
                orphans.add(name)

        return orphans

    async def prune(self, dry_run=False):
        """Remove packages from repository that aren't in the JSON config."""
        to_remove = self.get_orphaned_packages()

        if to_remove:
            if dry_run:
                logger.info(
                    f"Would remove {len(to_remove)} packages not in config:"
                )
                for package in sorted(to_remove):
                    logger.info(f"  - {package}")
                return
            else:
                logger.info(
                    f"Removing {len(to_remove)} packages not in config:"
                )
                for package in sorted(to_remove):
                    logger.info(f"  - {package}")

                # Get exact filenames from database before removing
                pkg_files_to_remove = {}
                for package in to_remove:
                    pkg = self.pkg_db.get_package(package)
                    if pkg:
                        pkg_files_to_remove[package] = pkg.filename

                # Batch remove from database
                await run_cmd(
                    "repo-remove",
                    "--sign",
                    str(self.db_tar),
                    *sorted(to_remove),
                )

                # Remove exact package files
                for package, filename in pkg_files_to_remove.items():
                    pkg_file = self.directory / filename
                    if pkg_file.exists():
                        pkg_file.unlink()
                    sig_file = pkg_file.with_name(pkg_file.name + ".sig")
                    if sig_file.exists():
                        sig_file.unlink()

                    # Remove from cache
                    try:
                        shutil.rmtree(self.aurutils_cache / package)
                    except FileNotFoundError:
                        pass
        else:
            logger.info("No packages to prune")

        # Remove unknown package files not in database
        known_files = set()
        for pkg in self.packages():
            alpm_pkg = self.pkg_db.get_package(pkg.name)
            if alpm_pkg:
                known_files.add(alpm_pkg.filename)
        unknown_files = []
        for path in self.directory.iterdir():
            if path.name.endswith(".pkg.tar.zst") or path.name.endswith(
                ".pkg.tar.xz"
            ):
                if path.name not in known_files:
                    unknown_files.append(path)

        if unknown_files:
            if dry_run:
                logger.info(
                    f"Would remove {len(unknown_files)} unknown files:"
                )
                for path in sorted(unknown_files):
                    logger.info(f"  - {path.name}")
            else:
                logger.info(f"Removing {len(unknown_files)} unknown files:")
                for path in sorted(unknown_files):
                    logger.info(f"  - {path.name}")
                    path.unlink()
                    sig_file = path.with_name(path.name + ".sig")
                    if sig_file.exists():
                        sig_file.unlink()

        if dry_run:
            return

        # Clean up cache for packages that are no longer needed
        needed_packages = self._calculate_needed_packages()

        for path in self.aurutils_cache.iterdir():
            if path.name not in needed_packages:
                logger.debug(f"Clearing cache for {path.name}")
                shutil.rmtree(path)

        await self._sign_database()
        await self._pacsync()
        await self._paccache()
        await self.sync_to_nullsum_net()

    async def sync_to_nullsum_net(self):
        await run_cmd(
            "rsync",
            "-avz",
            "--progress",
            "--delete-after",
            "/var/cache/pacman/sol/",
            "nullsum.net:~/srv/http/repo/arch/x86_64",
            check=True,
        )

    async def remove(self, *packages):
        # Remove packages that are actually in the repository database
        repo_packages = {pkg.name for pkg in self.packages()}
        packages_to_remove = repo_packages.intersection(set(packages))

        if packages_to_remove:
            logger.info(f"Removing {len(packages_to_remove)} package(s):")
            for pkg in sorted(packages_to_remove):
                logger.info(f"  - {pkg}")

            # Remove from config (both aur and manual sections)
            self.config.remove_packages(*packages_to_remove)
            self.config.remove_no_update(*packages_to_remove)

            await run_cmd(
                "repo-remove",
                "--sign",
                str(self.db_tar),
                *sorted(packages_to_remove),
            )

            for package in packages_to_remove:
                pkg = self.pkg_db.get_package(package)
                if pkg:
                    pkg_file = self.directory / pkg.filename
                    if pkg_file.exists():
                        pkg_file.unlink()
                    sig_file = pkg_file.with_name(pkg_file.name + ".sig")
                    if sig_file.exists():
                        sig_file.unlink()
                try:
                    shutil.rmtree(self.aurutils_cache / package)
                except FileNotFoundError:
                    pass

        await self._sign_database()
        await self._pacsync()
        await self.sync_to_nullsum_net()

    @staticmethod
    async def _sign(path):
        path = pathlib.Path(path)
        dest = path.with_name(path.name + ".sig")
        await run_cmd(
            "gpg",
            "--yes",
            "--detach-sign",
            "--use-agent",
            "--no-armor",
            "--output",
            str(dest),
            str(path),
            check=True,
        )

    async def add(
        self,
        *packages,
        as_deps=False,
        as_explicit=False,
        as_noaur=False,
        as_manual=False,
        as_no_manual=False,
        parents=None,
        makepkg_flags=None,
    ):
        await self._sign_database()
        await self._pacsync()

        # Build makepkg args from flags
        makepkg_args = []
        if makepkg_flags:
            makepkg_args.extend(makepkg_flags)

        # Note: parents parameter is deprecated with the new list-based config format
        if parents:
            logger.warning("--parent flag is deprecated and will be ignored")

        # Get all packages currently in the repository database
        repo_packages = {pkg.name for pkg in self.packages()}
        repo_pkgbases = {pkg.pkgbase for pkg in self.packages()}

        packages_and_deps = set()
        new_packages = []
        new_files = []
        already_exists = []

        reclassified = []

        for package in packages:
            maybe_path = pathlib.Path(package)
            # Check if this looks like a file path but doesn't exist
            is_file_like = "/" in package or package.endswith(
                (".pkg.tar.zst", ".pkg.tar.xz", ".pkg.tar.gz")
            )
            if is_file_like and not maybe_path.is_file():
                logger.error(f"Package file not found: {package}")
                continue
            if maybe_path.is_file():
                # Handle package file
                await run_cmd(
                    "sudo",
                    "cp",
                    str(maybe_path),
                    str(self.directory),
                    check=True,
                )
                await run_cmd(
                    "repo-add",
                    "--sign",
                    str(self.db_tar),
                    str(maybe_path),
                    check=True,
                )
                await self._sign(self.directory / maybe_path.name)

                # Extract package name and pkgbase using pyalpm
                try:
                    pkg_file = self.pkg_db.load_package_file(maybe_path)
                    pkg_name_only = pkg_file.name
                    pkgbase = pkg_file.base or pkg_name_only
                except Exception as e:
                    logger.error(
                        f"Failed to load package file {maybe_path}: {e}"
                    )
                    continue

                new_files.append(pkg_name_only)

                # Handle aur section (explicit vs dependency)
                if as_explicit:
                    # MUST be in aur
                    self.config.add_packages(pkgbase)
                elif as_deps or as_noaur:
                    # MUST NOT be in aur
                    self.config.remove_packages(pkgbase)
                # else: Default: no change (existing file) or don't add (new file)

                # Handle manual section (manual vs updateable)
                if as_manual:
                    # MUST be in manual
                    self.config.add_no_update(pkgbase)
                elif as_no_manual:
                    # MUST NOT be in manual
                    self.config.remove_no_update(pkgbase)
                else:
                    # Default: add to manual only if NEW file package
                    if pkgbase not in repo_pkgbases:
                        self.config.add_no_update(pkgbase)
            else:
                # Check if package already exists in repository
                if package in repo_packages:
                    # Package is already built and in the repo
                    is_in_aur = package in self.config.aur
                    is_in_manual = package in self.config.manual
                    changed = False

                    # Handle aur section (explicit vs dependency)
                    if as_explicit:
                        # MUST be in aur
                        if not is_in_aur:
                            self.config.add_packages(package)
                            reclassified.append(
                                (package, "dependency", "explicit")
                            )
                            changed = True
                    elif as_deps or as_noaur:
                        # MUST NOT be in aur
                        if is_in_aur:
                            self.config.remove_packages(package)
                            reclassified.append(
                                (package, "explicit", "dependency")
                            )
                            changed = True
                    # else: no change to aur section

                    # Handle manual section (manual vs updateable)
                    if as_manual:
                        # MUST be in manual
                        if not is_in_manual:
                            self.config.add_no_update(package)
                            changed = True
                    elif as_no_manual:
                        # MUST NOT be in manual
                        if is_in_manual:
                            self.config.remove_no_update(package)
                            changed = True
                    # else: no change to manual section

                    if not changed:
                        already_exists.append(package)
                    continue

                # Handle new AUR package
                # Just add the package itself - aur sync will handle dependencies
                # and makepkg/pacman will properly check if they're satisfied by provides
                packages_and_deps.add(package)
                new_packages.append(package)

                # Handle aur section (explicit vs dependency)
                if as_explicit:
                    # MUST be in aur
                    self.config.add_packages(package)
                elif as_deps or as_noaur:
                    # MUST NOT be in aur (don't add)
                    pass
                else:
                    # Default: add to aur
                    self.config.add_packages(package)

                # Handle manual section (manual vs updateable)
                if as_manual:
                    # MUST be in manual
                    self.config.add_no_update(package)
                elif as_no_manual:
                    # MUST NOT be in manual (don't add)
                    pass
                # else: default is to not add to manual for new AUR packages

        if packages_and_deps:
            await AURCLI.sync(
                list(packages_and_deps),
                database=self.name,
                makepkg_args=makepkg_args,
                no_ver=True,
            )

        # Report reclassified packages
        if reclassified:
            logger.info(f"Reclassified {len(reclassified)} package(s):")
            for package, old_cat, new_cat in sorted(reclassified):
                logger.info(f"  - {package}: {old_cat} â†’ {new_cat}")

        # Report packages that already exist
        if already_exists:
            logger.info(
                f"Skipping {len(already_exists)} package(s) already in repository:"
            )
            for package in sorted(already_exists):
                logger.info(f"  - {package}")

        # Exit early if nothing to add or reclassify
        if not new_packages and not new_files and not reclassified:
            if already_exists:
                logger.info("No new packages to add.")
            return

        # Only sync if we actually built/added packages or added files
        if new_packages or new_files:
            await self._sign_database()
            await self._pacsync()
            await self.sync_to_nullsum_net()
        elif reclassified:
            # Only update config file, no need to sync repo
            logger.info("Configuration updated (no repository sync needed)")

    @staticmethod
    def _is_vcs_package(pkg_name):
        """Check if a package is a VCS package based on its name."""
        vcs_suffixes = ("-git", "-svn", "-hg", "-bzr", "-cvs", "-darcs")
        return pkg_name.endswith(vcs_suffixes)

    async def _get_vcs_pkgver(self, pkgbase):
        """
        Fetch VCS package and run pkgver() to get the current version.
        Returns the version string or None if it fails.
        """
        pkg_dir = self.aurutils_cache / pkgbase

        # Fetch the package and reset to remote
        if not await AURCLI.fetch(
            pkgbase, cwd=self.aurutils_cache, reset=True
        ):
            return None

        # Run makepkg -od to execute pkgver() function
        # -o: download/update sources
        # -d: skip dependency checks
        makepkg_result = await run_cmd(
            "makepkg", "-od", cwd=str(pkg_dir), capture_output=True
        )

        if makepkg_result.returncode != 0:
            return None

        # Parse the version from PKGBUILD
        # Source the PKGBUILD and extract pkgver and pkgrel
        bash_cmd = "source PKGBUILD && echo ${pkgver}-${pkgrel}"
        version_result = await run_cmd(
            "bash", "-c", bash_cmd, cwd=str(pkg_dir), capture_output=True
        )

        if version_result.returncode != 0:
            return None

        version = version_result.stdout.decode().strip()
        return version if version else None

    async def _parse_srcinfo(self, pkg_name) -> DependencyInfo | None:
        """
        Parse .SRCINFO for a package and return dependency information.
        Returns DependencyInfo dataclass or None if parsing fails.
        """
        srcinfo_path = self.aurutils_cache / pkg_name / ".SRCINFO"
        if not srcinfo_path.exists():
            return None

        depends = []
        optdepends = []
        makedepends = []

        try:
            with open(srcinfo_path, "r") as f:
                for line in f:
                    line = line.strip()
                    if line.startswith("depends = "):
                        dep = line.split("=", 1)[1].strip()
                        depends.append(strip_version(dep))
                    elif line.startswith("optdepends = "):
                        dep = line.split("=", 1)[1].strip()
                        optdepends.append(strip_version(dep))
                    elif line.startswith("makedepends = "):
                        dep = line.split("=", 1)[1].strip()
                        makedepends.append(strip_version(dep))
        except Exception:
            return None

        return DependencyInfo(
            depends=depends, optdepends=optdepends, makedepends=makedepends
        )

    async def _calculate_post_update_tree(
        self, pkg_updates: dict[str, PackageUpdateInfo]
    ) -> set[str]:
        """
        Calculate what the dependency tree would be after applying updates.

        Args:
            pkg_updates: Dict mapping package names to PackageUpdateInfo

        Returns:
            Set of package names that would be needed after updates
        """
        # Start with explicit packages from config
        needed = set(self.config.get_explicit_packages())

        # Process packages until we've expanded all dependencies
        to_process = needed.copy()
        processed = set()

        while to_process:
            pkg_name = to_process.pop()
            if pkg_name in processed:
                continue
            processed.add(pkg_name)

            # Get dependencies - use updated deps if available, otherwise current repo
            if pkg_name in pkg_updates and pkg_updates[pkg_name].deps:
                deps_info = pkg_updates[pkg_name].deps
                all_deps = (
                    deps_info.depends
                    + deps_info.optdepends
                    + deps_info.makedepends
                )
            else:
                # Use current repo dependencies
                pkg = self.pkg_db.get_package(pkg_name)
                if not pkg:
                    continue
                all_deps = []
                for dep in pkg.depends:
                    all_deps.append(strip_version(dep))
                for optdep in pkg.optdepends:
                    all_deps.append(strip_version(optdep))
                for makedep in pkg.makedepends:
                    all_deps.append(strip_version(makedep))

            # Add dependencies to needed set
            for dep_name in all_deps:
                if dep_name and dep_name not in needed:
                    needed.add(dep_name)
                    to_process.add(dep_name)

        return needed

    async def _check_package_updates(
        self, pkgbases_to_check: set[str]
    ) -> dict[str, PackageUpdateInfo]:
        """
        Check which pkgbases have updates available.

        Args:
            pkgbases_to_check: Set of pkgbases to check

        Returns:
            Dict mapping pkgbase to PackageUpdateInfo for pkgbases with updates
        """
        pkg_updates = {}

        for pkgbase in tqdm.tqdm(pkgbases_to_check):
            # Find current version from any package with this pkgbase
            current_version = None
            for pkg in self.pkg_db.sol_db.pkgcache:
                if pkg.base == pkgbase:
                    current_version = pkg.version
                    break

            if not current_version:
                continue

            is_vcs = self._is_vcs_package(pkgbase)

            if is_vcs:
                # Check VCS package for new commits
                new_version = await self._get_vcs_pkgver(pkgbase)
                if not new_version:
                    logger.warning(
                        f"Could not determine VCS version for {pkgbase}, skipping"
                    )
                    continue

                # VCS versions (e.g. git commit hashes) don't have
                # meaningful ordering - any change means an update
                if new_version != current_version:
                    logger.info(
                        f"  {pkgbase}: {current_version} -> {new_version}"
                    )
                    deps = await self._parse_srcinfo(pkgbase)
                    pkg_updates[pkgbase] = PackageUpdateInfo(
                        name=pkgbase,
                        current_version=current_version,
                        new_version=new_version,
                        has_update=True,
                        is_vcs=True,
                        deps=deps,
                    )
                else:
                    logger.debug(
                        f"  {pkgbase}: up to date ({current_version})"
                    )
            else:
                # Check non-VCS package by fetching PKGBUILD
                if not await AURCLI.fetch(
                    pkgbase, cwd=self.aurutils_cache, reset=True
                ):
                    logger.error(f"Failed to fetch {pkgbase}")
                    continue

                # Parse .SRCINFO to get version
                srcinfo_path = self.aurutils_cache / pkgbase / ".SRCINFO"
                if not srcinfo_path.exists():
                    continue

                try:
                    pkgver = pkgrel = epoch = None
                    with open(srcinfo_path, "r") as f:
                        for line in f:
                            stripped = line.strip()
                            if stripped.startswith("epoch = "):
                                epoch = stripped.split("=", 1)[1].strip()
                            elif stripped.startswith("pkgver = "):
                                pkgver = stripped.split("=", 1)[1].strip()
                            elif stripped.startswith("pkgrel = "):
                                pkgrel = stripped.split("=", 1)[1].strip()
                            elif stripped.startswith("pkgname = "):
                                # Past header section, stop parsing
                                break
                    if pkgver and pkgrel:
                        new_version = f"{epoch}:{pkgver}-{pkgrel}" if epoch else f"{pkgver}-{pkgrel}"
                    else:
                        new_version = None
                except Exception:
                    continue

                if not new_version:
                    continue

                # Compare versions
                if pyalpm.vercmp(new_version, current_version) > 0:
                    # Has update - parse dependencies
                    deps = await self._parse_srcinfo(pkgbase)
                    pkg_updates[pkgbase] = PackageUpdateInfo(
                        name=pkgbase,
                        current_version=current_version,
                        new_version=new_version,
                        has_update=True,
                        is_vcs=False,
                        deps=deps,
                    )

        return pkg_updates

    def _filter_updates_to_needed(
        self,
        pkg_updates: dict[str, PackageUpdateInfo],
        needed_packages: set[str],
    ) -> set[str]:
        """
        Filter package updates to only those that will still be needed after the update.

        Args:
            pkg_updates: Dict of PackageUpdateInfo
            needed_packages: Set of packages that will be needed post-update

        Returns:
            Set of package names to actually update
        """
        packages_to_update = set()

        for pkg_name, update_info in pkg_updates.items():
            if update_info.has_update and pkg_name in needed_packages:
                packages_to_update.add(pkg_name)

        return packages_to_update

    def _sort_by_dependency_depth(
        self, packages: set[str], pkg_updates: dict[str, PackageUpdateInfo]
    ) -> list[str]:
        """
        Sort packages by dependency depth (deepest first).

        Args:
            packages: Set of package names to sort
            pkg_updates: Dict of PackageUpdateInfo (for dependency information)

        Returns:
            List of package names sorted deepest-first
        """
        distances = {}
        parents = self.config.aur
        sol_db = self.pkg_db.sol_db

        def calculate_distance(pkg, visited=None):
            if visited is None:
                visited = set()
            if pkg in visited:
                return 0
            if pkg in distances:
                return distances[pkg]

            visited.add(pkg)

            # If it's a parent (explicit package), distance is 0
            if pkg in parents:
                distances[pkg] = 0
                return 0

            # Find all packages that depend on this package
            max_dist = 0

            # Check which packages depend on this one
            for potential_parent in packages:
                # Use updated dependencies if available, otherwise current repo
                if (
                    potential_parent in pkg_updates
                    and pkg_updates[potential_parent].deps
                ):
                    deps_info = pkg_updates[potential_parent].deps
                    all_deps = deps_info.depends + deps_info.optdepends
                else:
                    # Use current repo dependencies
                    parent_pkg = sol_db.get_pkg(potential_parent)
                    if not parent_pkg:
                        continue
                    all_deps = []
                    for dep in parent_pkg.depends:
                        all_deps.append(strip_version(dep))
                    for optdep in parent_pkg.optdepends:
                        all_deps.append(strip_version(optdep))

                # Check if pkg is in this package's dependencies
                if pkg in all_deps:
                    dist = (
                        calculate_distance(potential_parent, visited.copy())
                        + 1
                    )
                    max_dist = max(max_dist, dist)

            distances[pkg] = max_dist
            return max_dist

        # Calculate distances for all packages
        for pkg in packages:
            calculate_distance(pkg)

        # Sort packages by distance (highest first - deepest dependencies first)
        # Ensure each package appears only once
        seen = set()
        sorted_packages = []
        for pkg in sorted(
            packages, key=lambda p: distances.get(p, 0), reverse=True
        ):
            if pkg not in seen:
                seen.add(pkg)
                sorted_packages.append(pkg)

        return sorted_packages

    async def _build_packages(
        self,
        sorted_packages: list[str],
        pkg_updates: dict[str, PackageUpdateInfo],
        makepkg_args: list[str],
    ) -> tuple[set[str], list[str]]:
        """
        Build packages in order, tracking failures and skips.

        Args:
            sorted_packages: List of packages sorted by dependency depth
            pkg_updates: Dict of PackageUpdateInfo (to know which are VCS)
            makepkg_args: makepkg flags to pass

        Returns:
            Tuple of (failed_packages: set, skipped_packages: list)
        """
        failed_packages = set()
        skipped_packages = []
        sol_db = self.pkg_db.sol_db

        def has_failed_dependency(pkg_name, failed_set):
            """Check if pkg_name depends on any package in failed_set."""
            pkg = sol_db.get_pkg(pkg_name)
            if not pkg:
                return False

            for dep in pkg.depends:
                dep_name = strip_version(dep)
                if dep_name in failed_set:
                    return True

            for optdep in pkg.optdepends:
                dep_name = strip_version(optdep)
                if dep_name in failed_set:
                    return True

            return False

        # Split into VCS and non-VCS
        vcs_packages = [
            pkg
            for pkg in sorted_packages
            if pkg in pkg_updates and pkg_updates[pkg].is_vcs
        ]
        non_vcs_packages = [
            pkg
            for pkg in sorted_packages
            if pkg not in pkg_updates or not pkg_updates[pkg].is_vcs
        ]

        # Build non-VCS packages
        if non_vcs_packages:
            logger.info(
                f"Updating {len(non_vcs_packages)} non-VCS package(s)..."
            )
            for pkg_name in non_vcs_packages:
                if has_failed_dependency(pkg_name, failed_packages):
                    logger.warning(
                        f"  Skipping {pkg_name} (depends on failed package)"
                    )
                    skipped_packages.append(pkg_name)
                    continue

                logger.debug(f"  Building {pkg_name}...")
                success = await AURCLI.sync(
                    [pkg_name],
                    database=self.name,
                    cwd=self.aurutils_cache,
                    makepkg_args=makepkg_args,
                    no_ver=True,
                )
                if not success:
                    logger.error(f"  Failed to build {pkg_name}")
                    failed_packages.add(pkg_name)

        # Build VCS packages
        if vcs_packages:
            logger.info(f"Building {len(vcs_packages)} VCS package(s)...")
            for pkg_name in vcs_packages:
                if has_failed_dependency(pkg_name, failed_packages):
                    logger.warning(
                        f"  Skipping {pkg_name} (depends on failed package)"
                    )
                    skipped_packages.append(pkg_name)
                    continue

                logger.debug(f"  Building {pkg_name}...")
                pkg_dir = self.aurutils_cache / pkg_name
                success = await AURCLI.build(
                    database=self.name,
                    cwd=pkg_dir,
                    makepkg_args=makepkg_args,
                    force=True,
                )
                if not success:
                    logger.error(f"  Failed to build {pkg_name}")
                    failed_packages.add(pkg_name)

        return failed_packages, skipped_packages

    def _report_failures_and_skips(
        self, failed_packages: set[str], skipped_packages: list[str]
    ):
        """Report build failures and skipped packages."""
        if failed_packages:
            logger.error(f"{len(failed_packages)} package(s) failed to build:")
            for pkg in sorted(failed_packages):
                logger.error(f"  - {pkg}")

        if skipped_packages:
            logger.warning(
                f"{len(skipped_packages)} package(s) skipped due to failed dependencies:"
            )
            for pkg in sorted(skipped_packages):
                logger.warning(f"  - {pkg}")

    async def update(self, packages=None, makepkg_flags=None):
        """
        Update packages in the repository.

        Args:
            packages: Optional list of specific packages to update
            makepkg_flags: Optional list of makepkg flags (--skippgpcheck, etc.)
        """
        # Initialization
        await self.cleanup()
        await self._sign_database()
        await self._pacsync()
        self.aurutils_cache.mkdir(exist_ok=True, parents=True)

        makepkg_args = []
        if makepkg_flags:
            makepkg_args.extend(makepkg_flags)

        sync_dbs = self.pkg_db.get_official_dbs()

        # Check packages now in official repos
        packages_in_official_repos = set()
        repo_packages = {pkg.name for pkg in self.packages()}

        for pkg_name in repo_packages:
            for sync_db in sync_dbs:
                if sync_db.name != self.name and sync_db.get_pkg(pkg_name):
                    packages_in_official_repos.add(pkg_name)
                    break

        if packages_in_official_repos:
            # Separate manual packages (warn only) from non-manual (remove)
            manual_in_official = (
                packages_in_official_repos & self.config.manual
            )
            to_remove = packages_in_official_repos - self.config.manual

            if manual_in_official:
                logger.warning(
                    f"Found {len(manual_in_official)} manual package(s) now in official repos (not removing):"
                )
                for pkg in sorted(manual_in_official):
                    logger.warning(f"  - {pkg}")

            if to_remove:
                logger.info(
                    f"Found {len(to_remove)} package(s) now in official repos, removing from sol:"
                )
                for pkg in sorted(to_remove):
                    logger.info(f"  - {pkg}")
                await self.remove(*to_remove)
                repo_packages = {pkg.name for pkg in self.packages()}

        # Determine which pkgbases to check
        # Build mapping of pkgbase -> packages and package -> pkgbase
        pkgbase_to_packages = {}
        package_to_pkgbase = {}
        for pkg in self.packages():
            package_to_pkgbase[pkg.name] = pkg.pkgbase
            if pkg.pkgbase not in pkgbase_to_packages:
                pkgbase_to_packages[pkg.pkgbase] = []
            pkgbase_to_packages[pkg.pkgbase].append(pkg.name)

        repo_pkgbases = set(pkgbase_to_packages.keys())

        if packages:
            # Resolve package names to pkgbases
            pkgbases_to_check = set()
            not_in_repo = set()
            for pkg in packages:
                if pkg in package_to_pkgbase:
                    pkgbases_to_check.add(package_to_pkgbase[pkg])
                elif pkg in repo_pkgbases:
                    # User passed a pkgbase directly
                    pkgbases_to_check.add(pkg)
                else:
                    not_in_repo.add(pkg)
            if not_in_repo:
                logger.warning(
                    f"{len(not_in_repo)} package(s) not in repository:"
                )
                for pkg in sorted(not_in_repo):
                    logger.warning(f"  - {pkg}")
        else:
            pkgbases_to_check = repo_pkgbases - self.config.manual

        # STEP 1: Check all pkgbases for updates
        logger.info("Checking for package updates...")
        pkg_updates: dict[
            str, PackageUpdateInfo
        ] = await self._check_package_updates(pkgbases_to_check)

        if not pkg_updates:
            logger.info("No updates available")
            return

        # Print what we found
        logger.info(f"Found updates for {len(pkg_updates)} package(s):")
        for pkg_name, info in sorted(pkg_updates.items()):
            logger.info(
                f"  {pkg_name}: {info.current_version} -> {info.new_version}"
            )

        # STEP 2: Calculate post-update dependency tree
        logger.debug("Calculating post-update dependency tree...")
        needed_after_update: set[str] = await self._calculate_post_update_tree(
            pkg_updates
        )

        # STEP 3: Filter to packages that will still be needed
        logger.debug(
            f"Filtering updates (needed after update: {len(needed_after_update)} packages)..."
        )
        packages_to_update: set[str] = self._filter_updates_to_needed(
            pkg_updates, needed_after_update
        )

        if not packages_to_update:
            logger.info(
                "No packages to update (all updates would result in orphaned packages)"
            )
            return

        # STEP 4: Sort by dependency depth
        sorted_packages: list[str] = self._sort_by_dependency_depth(
            packages_to_update, pkg_updates
        )

        # STEP 5: Build packages
        failed: set[str]
        skipped: list[str]
        failed, skipped = await self._build_packages(
            sorted_packages, pkg_updates, makepkg_args
        )

        # Cleanup and sync
        if sorted_packages:
            await self._sign_database()
            await self._pacsync()
            await self._paccache()
            await self.sync_to_nullsum_net()

        # Report results
        self._report_failures_and_skips(failed, skipped)

        return

    async def _old_update_REMOVE_ME(self, packages=None, makepkg_flags=None):
        """OLD VERSION - TO BE REMOVED AFTER TESTING"""
        await self.cleanup()
        await self._pacsync()
        self.aurutils_cache.mkdir(exist_ok=True, parents=True)

        # Build makepkg args from flags
        makepkg_args = []
        if makepkg_flags:
            makepkg_args.extend(makepkg_flags)

        sync_dbs = self.pkg_db.get_official_dbs()

        # Step 1: Remove packages now in official repos
        packages_in_official_repos = set()
        repo_packages = {pkg.name for pkg in self.packages()}

        for pkg_name in repo_packages:
            for sync_db in sync_dbs:
                if sync_db.name != self.name and sync_db.get_pkg(pkg_name):
                    packages_in_official_repos.add(pkg_name)
                    break

        if packages_in_official_repos:
            print(
                f"Found {len(packages_in_official_repos)} packages now in official repos, removing from sol:"
            )
            for pkg in sorted(packages_in_official_repos):
                print(f"  - {pkg}")
            await self.remove(*packages_in_official_repos)
            repo_packages = {
                pkg.name for pkg in self.packages()
            }  # Refresh after removal

        # Step 2: Check all packages for updates and fetch PKGBUILDs
        print("Checking for package updates...")

        # Determine which packages to check
        if packages:
            # Only check specified packages
            packages_to_check = set(packages) & repo_packages
            not_in_repo = set(packages) - repo_packages
            if not_in_repo:
                print(
                    f"Warning: {len(not_in_repo)} package(s) not in repository:"
                )
                for pkg in sorted(not_in_repo):
                    print(f"  - {pkg}")
        else:
            # Check all packages in repo (excluding manual)
            packages_to_check = repo_packages - self.config.manual

        # Check VCS packages
        vcs_packages_to_check = [
            p for p in packages_to_check if self._is_vcs_package(p)
        ]
        for pkg_name in vcs_packages_to_check:
            sol_pkg = self.pkg_db.get_package(pkg_name)
            if not sol_pkg:
                continue

            # Check only required dependencies
            for dep in sol_pkg.depends:
                dep_name = strip_version(dep)

                if not dep_name:
                    continue

                satisfied = False

                # Check sol repo
                if self.pkg_db.get_package(dep_name):
                    satisfied = True
                else:
                    # Check provides in sol repo
                    for p in self.pkg_db.sol_db.pkgcache:
                        for prov in p.provides:
                            prov_name = strip_version(prov)
                            if prov_name == dep_name:
                                satisfied = True
                                break
                        if satisfied:
                            break

                # Check sync repos (official repos)
                if not satisfied:
                    for sync_db in sync_dbs:
                        if sync_db.get_pkg(dep_name):
                            satisfied = True
                            break
                        # Check provides in sync repos
                        for p in sync_db.pkgcache:
                            for prov in p.provides:
                                prov_name = strip_version(prov)
                                if prov_name == dep_name:
                                    satisfied = True
                                    break
                            if satisfied:
                                break
                        if satisfied:
                            break

                if not satisfied:
                    missing_deps.add(dep_name)

        if missing_deps:
            print(
                f"Found {len(missing_deps)} missing dependencies, adding to update list:"
            )
            for dep in sorted(missing_deps):
                print(f"  - {dep}")
            packages_to_update.update(missing_deps)

        if not packages_to_update:
            print("No AUR packages to update")
            return

        # Calculate maximum distance from parents for each package
        # This ensures dependencies are updated before packages that depend on them
        distances = {}
        parents = self.config.aur

        # Create pyalpm handle once for efficiency
        try:
            sol_db = self.pkg_db.sol_db
        except Exception:
            raise
            # If we can't get the database, just use unsorted packages
            sorted_packages = list(packages_to_update)
            await run_cmd(
                "aur",
                "sync",
                "--no-view",
                "--no-confirm",
                f"--database={self.name}",
                "--",
                *sorted_packages,
                cwd=str(self.aurutils_cache),
                check=True,
            )
            await self._pacsync()
            await self._paccache()
            await self.sync_to_nullsum_net()
            return

        def calculate_distance(pkg, visited=None):
            if visited is None:
                visited = set()
            if pkg in visited:
                return 0
            if pkg in distances:
                return distances[pkg]

            visited.add(pkg)

            # If it's a parent, distance is 0
            if pkg in parents:
                distances[pkg] = 0
                return 0

            # Find all packages that depend on this package
            max_dist = 0

            # Check which packages depend on this one
            for potential_parent in packages_to_update:
                parent_pkg = sol_db.get_pkg(potential_parent)
                if parent_pkg:
                    # Check if pkg is in this package's dependencies
                    for dep in parent_pkg.depends:
                        dep_name = strip_version(dep)
                        if dep_name == pkg:
                            dist = (
                                calculate_distance(
                                    potential_parent, visited.copy()
                                )
                                + 1
                            )
                            max_dist = max(max_dist, dist)
                    for optdep in parent_pkg.optdepends:
                        dep_name = strip_version(optdep)
                        if dep_name == pkg:
                            dist = (
                                calculate_distance(
                                    potential_parent, visited.copy()
                                )
                                + 1
                            )
                            max_dist = max(max_dist, dist)

            distances[pkg] = max_dist
            return max_dist

        # Calculate distances for all packages
        for pkg in packages_to_update:
            calculate_distance(pkg)

        # Sort packages by distance (highest first - deepest dependencies first)
        # Ensure each package appears only once
        seen = set()
        sorted_packages = []
        for pkg in sorted(
            packages_to_update, key=lambda p: distances.get(p, 0), reverse=True
        ):
            if pkg not in seen:
                seen.add(pkg)
                sorted_packages.append(pkg)

        # Split packages into VCS and non-VCS
        vcs_packages = [
            pkg for pkg in sorted_packages if self._is_vcs_package(pkg)
        ]
        non_vcs_packages = [
            pkg for pkg in sorted_packages if not self._is_vcs_package(pkg)
        ]

        # For VCS packages, check if they need updating by comparing versions
        vcs_to_build = []
        if vcs_packages:
            print(
                f"Checking {len(vcs_packages)} VCS package(s) for updates..."
            )
            for pkg_name in vcs_packages:
                # Get current version in sol repo
                sol_pkg = self.pkg_db.get_package(pkg_name)
                if not sol_pkg:
                    # Package not in repo yet, add it
                    vcs_to_build.append(pkg_name)
                    continue

                sol_version = sol_pkg.version

                # Get current VCS version by running pkgver()
                current_version = await self._get_vcs_pkgver(pkg_name)
                if not current_version:
                    print(
                        f"  Warning: Could not determine version for {pkg_name}, skipping"
                    )
                    continue

                # VCS versions (e.g. git commit hashes) don't have
                # meaningful ordering - any change means an update
                if current_version != sol_version:
                    print(f"  {pkg_name}: {sol_version} -> {current_version}")
                    vcs_to_build.append(pkg_name)
                else:
                    print(f"  {pkg_name}: up to date ({sol_version})")

        # Helper function to check if a package depends on any failed packages
        def has_failed_dependency(pkg_name, failed_set):
            """Check if pkg_name depends on any package in failed_set."""
            pkg = sol_db.get_pkg(pkg_name)
            if not pkg:
                return False

            for dep in pkg.depends:
                dep_name = strip_version(dep)
                if dep_name in failed_set:
                    return True

            for optdep in pkg.optdepends:
                dep_name = strip_version(optdep)
                if dep_name in failed_set:
                    return True

            return False

        # Sync non-VCS packages one at a time to avoid stopping on failure
        failed_packages = set()
        skipped_packages = []

        if non_vcs_packages:
            print(f"Updating {len(non_vcs_packages)} non-VCS package(s)...")
            for pkg_name in non_vcs_packages:
                # Skip if this package depends on a failed package
                if has_failed_dependency(pkg_name, failed_packages):
                    print(f"  Skipping {pkg_name} (depends on failed package)")
                    skipped_packages.append(pkg_name)
                    continue

                cmd = [
                    "aur",
                    "sync",
                    "--no-view",
                    "--no-confirm",
                    f"--database={self.name}",
                ]
                if makepkg_args:
                    cmd.extend(["--margs", *makepkg_args])
                cmd.extend(["--", pkg_name])
                result = await run_cmd(
                    *cmd,
                    cwd=str(self.aurutils_cache),
                )
                if result.returncode != 0:
                    print(f"  Failed to build {pkg_name}")
                    failed_packages.add(pkg_name)

        # Build VCS packages that need updating one at a time
        if vcs_to_build:
            print(f"Building {len(vcs_to_build)} VCS package(s)...")
            for pkg_name in vcs_to_build:
                # Skip if this package depends on a failed package
                if has_failed_dependency(pkg_name, failed_packages):
                    print(f"  Skipping {pkg_name} (depends on failed package)")
                    skipped_packages.append(pkg_name)
                    continue

                # aur build must be run from inside the package directory
                pkg_dir = self.aurutils_cache / pkg_name
                cmd = [
                    "aur",
                    "build",
                    "--force",
                    f"--database={self.name}",
                ]
                if makepkg_args:
                    cmd.extend(["--margs", *makepkg_args])
                result = await run_cmd(
                    *cmd,
                    cwd=str(pkg_dir),
                )
                if result.returncode != 0:
                    print(f"  Failed to build {pkg_name}")
                    failed_packages.add(pkg_name)

        if non_vcs_packages or vcs_to_build:
            await self._pacsync()
            await self._paccache()
            await self.sync_to_nullsum_net()
        else:
            print("No packages to update")

        # Report summary of failures and skips
        if failed_packages:
            print(f"\n{len(failed_packages)} package(s) failed to build:")
            for pkg in sorted(failed_packages):
                print(f"  - {pkg}")

        if skipped_packages:
            print(
                f"\n{len(skipped_packages)} package(s) skipped due to failed dependencies:"
            )
            for pkg in sorted(skipped_packages):
                print(f"  - {pkg}")

        return

    async def _paccache(self):
        await run_cmd(
            "paccache",
            "--remove",
            "--keep",
            "1",
            "--cachedir",
            str(self.directory),
            check=True,
        )

    async def _pacsync(self):
        await run_cmd(
            "sudo",
            "pacsync",
            self.name,
            stderr=asyncio.subprocess.DEVNULL,
            check=True,
        )


def db_list(orphans=False, as_json=False, show_all=False, depends_on=None):
    database = Database(DATABASE)

    if depends_on:
        reverse_deps = _find_reverse_deps(database, depends_on)
        for pkg in sorted(reverse_deps):
            print(pkg)
        return

    if orphans:
        # Show orphaned packages
        orphan_list = database.get_orphaned_packages()
        if orphan_list:
            for package in sorted(orphan_list):
                print(package)
        return

    if as_json:
        # Output full package info as JSON for packages in config
        try:
            pkg_db = PackageDatabase(DATABASE)
        except Exception as e:
            print(f"Error: Coul not access database: {e}", file=sys.stderr)
            return

        # Get packages to show
        if show_all:
            # Show all packages in sol database
            packages_to_show = [pkg.name for pkg in database.packages()]
        else:
            # Show only packages from config
            packages_to_show = list(database.config.get_explicit_packages())

        result = {}
        for pkg_name in packages_to_show:
            pkg = pkg_db.get_package(pkg_name)
            if not pkg:
                continue

            # Get dependencies (only those in sol)
            depends = []
            for dep in pkg.depends:
                dep_name = strip_version(dep)
                if dep_name and pkg_db.get_package(dep_name):
                    depends.append(dep_name)

            # Get optional dependencies (only those in sol)
            optdepends = []
            for optdep in pkg.optdepends:
                dep_name = strip_version(optdep)
                if dep_name and pkg_db.get_package(dep_name):
                    optdepends.append(dep_name)

            # Get makedepends (only those in sol)
            makedepends = []
            for makedep in pkg.makedepends:
                dep_name = strip_version(makedep)
                if dep_name and pkg_db.get_package(dep_name):
                    makedepends.append(dep_name)

            pkg_info = {}
            if depends:
                pkg_info["depends"] = sorted(depends)
            if optdepends:
                pkg_info["optdepends"] = sorted(optdepends)
            if makedepends:
                pkg_info["makedepends"] = sorted(makedepends)

            result[pkg_name] = pkg_info

        print(json.dumps(result, indent=2))
        return

    # Default: list package names
    for package in database.packages():
        print(package.name)


def db_show_deps(packages=None):
    """Show dependencies for packages in the sol database as JSON."""
    database = Database(DATABASE)

    # Get list of packages to query
    if packages:
        package_list = set(packages)
    else:
        # Get all packages from the database
        package_list = {pkg.name for pkg in database.packages()}

    if not package_list:
        print("{}")
        return

    # Initialize pyalpm
    pkg_db = PackageDatabase(DATABASE)

    result = {"packages": {}}

    # Query each package
    for pkg_name in package_list:
        pkg = pkg_db.get_package(pkg_name)
        if not pkg:
            continue

        # Get required dependencies
        required = []
        for dep in pkg.depends:
            dep_name = strip_version(dep)
            if dep_name:
                required.append(dep_name)

        # Get optional dependencies
        optional = []
        for optdep in pkg.optdepends:
            dep_name = strip_version(optdep)
            if dep_name:
                optional.append(dep_name)

        result["packages"][pkg_name] = {
            "required": required,
            "optional": optional,
        }

    print(json.dumps(result, indent=2))


def db_check_deps():
    """Check for unsatisfied dependencies in the repo."""
    database = Database(DATABASE)

    try:
        pkg_db = PackageDatabase(DATABASE)
        sync_dbs = pkg_db.get_official_dbs()

        unsatisfied = {}

        for pkg in database.packages():
            sol_pkg = pkg_db.get_package(pkg.name)
            if not sol_pkg:
                continue

            pkg_unsatisfied = []

            # Check only required dependencies (not optdepends or makedepends)
            all_deps = list(sol_pkg.depends)

            for dep in all_deps:
                # Extract dependency name (strip version constraints)
                dep_name = strip_version(dep)

                if not dep_name:
                    continue

                satisfied = False

                # Check sol repo
                if pkg_db.get_package(dep_name):
                    satisfied = True
                else:
                    # Check provides in sol repo
                    for p in pkg_db.sol_db.pkgcache:
                        for prov in p.provides:
                            prov_name = strip_version(prov)
                            if prov_name == dep_name:
                                satisfied = True
                                break
                        if satisfied:
                            break

                # Check sync repos (official repos)
                if not satisfied:
                    for sync_db in sync_dbs:
                        if sync_db.get_pkg(dep_name):
                            satisfied = True
                            break
                        # Check provides in sync repos
                        for p in sync_db.pkgcache:
                            for prov in p.provides:
                                prov_name = strip_version(prov)
                                if prov_name == dep_name:
                                    satisfied = True
                                    break
                            if satisfied:
                                break
                        if satisfied:
                            break

                if not satisfied:
                    pkg_unsatisfied.append(dep_name)

            if pkg_unsatisfied:
                unsatisfied[pkg.name] = sorted(set(pkg_unsatisfied))

        if unsatisfied:
            print("Packages with unsatisfied dependencies:")
            for pkg_name, deps in sorted(unsatisfied.items()):
                print(f"\n{pkg_name}:")
                for dep in deps:
                    print(f"  - {dep}")
            return 1
        else:
            print("All dependencies are satisfied")
            return 0

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        import traceback

        traceback.print_exc()
        return 1


async def db_prune(dry_run=False):
    database = Database(DATABASE)
    await database.prune(dry_run=dry_run)


async def db_sync():
    database = Database(DATABASE)
    await database.sync_to_nullsum_net()


async def db_remove(packages):
    database = Database(DATABASE)
    await database.remove(*packages)


async def db_add(
    packages,
    delete=False,
    as_deps=False,
    as_explicit=False,
    as_noaur=False,
    as_manual=False,
    as_no_manual=False,
    parents=None,
    makepkg_flags=None,
):
    database = Database(DATABASE)
    await database.add(
        *packages,
        as_deps=as_deps,
        as_explicit=as_explicit,
        as_noaur=as_noaur,
        as_manual=as_manual,
        as_no_manual=as_no_manual,
        parents=parents,
        makepkg_flags=makepkg_flags,
    )
    if delete:
        for package in packages:
            path = pathlib.Path(package)
            if path.is_file():
                path.unlink()


async def db_update(packages=None, makepkg_flags=None):
    database = Database(DATABASE)
    await database.update(packages=packages, makepkg_flags=makepkg_flags)


def _find_reverse_deps(database, target_packages):
    """Find all packages in sol that depend on target_packages (anywhere in tree)."""
    target_set = set(target_packages)
    reverse_deps = set()

    # Get all packages and their full dependency info
    all_pkgs = {pkg.name: pkg for pkg in database.packages()}

    for pkg_name, pkg in all_pkgs.items():
        all_deps = set(pkg.depends + pkg.optdepends + pkg.makedepends)
        if all_deps & target_set:
            reverse_deps.add(pkg.pkgbase)

    return reverse_deps


async def db_rebuild(packages=None, makepkg_flags=None, depends_on=None):
    database = Database(DATABASE)
    await database._sign_database()
    await database._pacsync()
    database.aurutils_cache.mkdir(exist_ok=True, parents=True)

    makepkg_args = []
    if makepkg_flags:
        makepkg_args.extend(makepkg_flags)

    # Resolve package names to pkgbases
    package_to_pkgbase = {}
    for pkg in database.packages():
        package_to_pkgbase[pkg.name] = pkg.pkgbase

    pkgbases_to_rebuild = set()
    not_in_repo = []

    # Handle --dependson flag
    if depends_on:
        pkgbases_to_rebuild = _find_reverse_deps(database, depends_on)
        if not pkgbases_to_rebuild:
            logger.info(
                f"No packages in sol depend on: {', '.join(depends_on)}"
            )
            return
        logger.info(
            f"Found {len(pkgbases_to_rebuild)} package(s) depending on: {', '.join(depends_on)}"
        )
    elif packages:
        for pkg in packages:
            if pkg in package_to_pkgbase:
                pkgbases_to_rebuild.add(package_to_pkgbase[pkg])
            elif pkg in {p.pkgbase for p in database.packages()}:
                pkgbases_to_rebuild.add(pkg)
            else:
                not_in_repo.append(pkg)

        if not_in_repo:
            logger.warning(f"{len(not_in_repo)} package(s) not in repository:")
            for pkg in sorted(not_in_repo):
                logger.warning(f"  - {pkg}")
    else:
        logger.error("Must specify packages or --dependson")
        return

    if not pkgbases_to_rebuild:
        logger.info("No packages to rebuild")
        return

    logger.info(f"Rebuilding {len(pkgbases_to_rebuild)} package(s):")
    for pkg in sorted(pkgbases_to_rebuild):
        logger.info(f"  - {pkg}")

    for pkgbase in pkgbases_to_rebuild:
        # Fetch latest PKGBUILD
        if not await AURCLI.fetch(
            pkgbase, cwd=database.aurutils_cache, reset=True
        ):
            logger.error(f"Failed to fetch {pkgbase}")
            continue

        pkg_dir = database.aurutils_cache / pkgbase
        success = await AURCLI.build(
            database=database.name,
            cwd=pkg_dir,
            makepkg_args=makepkg_args,
            force=True,
        )
        if success:
            await database._sign_database()
        else:
            logger.error(f"Failed to build {pkgbase}")

    await database._sign_database()
    await database._pacsync()
    await database._paccache()
    await database.sync_to_nullsum_net()


async def db_sign(all_pkgs=False):
    database = Database(DATABASE)
    signed_count = 0
    for path in database.directory.iterdir():
        if not (
            path.name.endswith(".pkg.tar.zst")
            or path.name.endswith(".pkg.tar.xz")
        ):
            continue
        sig_path = path.with_name(path.name + ".sig")
        if all_pkgs or not sig_path.exists():
            logger.info(f"Signing {path.name}")
            await database._sign(path)
            signed_count += 1
    if signed_count:
        logger.info(f"Signed {signed_count} package(s)")
    else:
        logger.info("All packages already signed")
    logger.info("Signing database")
    await database._sign_database()
    await database.sync_to_nullsum_net()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="Enable verbose/debug output",
    )
    subparsers = parser.add_subparsers(
        dest="command", metavar="COMMAND", required=True
    )

    add_parser = subparsers.add_parser(
        "add", help="Add packages to the repository"
    )
    add_parser.add_argument(
        "--rm", action="store_true", help="Delete package file after adding"
    )
    add_parser.add_argument(
        "--asdeps", action="store_true", help="Add as dependency"
    )
    add_parser.add_argument(
        "--asexplicit", action="store_true", help="Add as explicit (default)"
    )
    add_parser.add_argument(
        "--noaur", action="store_true", help="Ensure not in aur section"
    )
    add_parser.add_argument(
        "--manual",
        action="store_true",
        help="Add to manual (no-update) section",
    )
    add_parser.add_argument(
        "--nomanual", action="store_true", help="Ensure not in manual section"
    )
    add_parser.add_argument(
        "--parent",
        action="append",
        dest="parents",
        help="Add as dependency of parent package (can be specified multiple times)",
    )
    add_parser.add_argument(
        "--skippgpcheck", action="store_true", help="Skip PGP signature checks"
    )
    add_parser.add_argument(
        "--skipinteg", action="store_true", help="Skip integrity checks"
    )
    add_parser.add_argument(
        "--skipchecksums", action="store_true", help="Skip checksum checks"
    )
    add_parser.add_argument("package", nargs="+")

    list_parser = subparsers.add_parser(
        "list", help="List packages in the repository"
    )
    list_parser.add_argument(
        "-o",
        "--orphans",
        action="store_true",
        help="Show orphaned packages only",
    )
    list_parser.add_argument(
        "--json",
        action="store_true",
        help="Output package dependencies as JSON",
    )
    list_parser.add_argument(
        "-a",
        "--all",
        action="store_true",
        help="Include all dependencies (not just sol packages)",
    )
    list_parser.add_argument(
        "--dependson",
        action="append",
        dest="depends_on",
        metavar="PKG",
        help="List packages that depend on PKG (can be repeated)",
    )

    prune_parser = subparsers.add_parser(
        "prune", help="Remove packages not in config"
    )
    prune_parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be removed without removing",
    )

    show_deps_parser = subparsers.add_parser(
        "show-deps", help="Show dependencies as JSON"
    )
    show_deps_parser.add_argument(
        "package",
        nargs="*",
        help="Specific packages (optional, defaults to all)",
    )

    subparsers.add_parser(
        "check-deps", help="Check for unsatisfied dependencies"
    )

    remove_parser = subparsers.add_parser(
        "remove", help="Remove packages from the repository"
    )
    remove_parser.add_argument("package", nargs="+")

    sign_parser = subparsers.add_parser("sign", help="Sign packages")
    sign_parser.add_argument("--all", action="store_true")

    subparsers.add_parser("sync", help="Sync repository to remote")

    update_parser = subparsers.add_parser("update", help="Update packages")
    update_parser.add_argument(
        "package", nargs="*", help="Specific packages to update (optional)"
    )
    update_parser.add_argument(
        "--skippgpcheck", action="store_true", help="Skip PGP signature checks"
    )
    update_parser.add_argument(
        "--skipinteg", action="store_true", help="Skip integrity checks"
    )
    update_parser.add_argument(
        "--skipchecksums", action="store_true", help="Skip checksum checks"
    )

    rebuild_parser = subparsers.add_parser("rebuild", help="Rebuild packages")
    rebuild_parser.add_argument(
        "package", nargs="*", help="Packages to rebuild"
    )
    rebuild_parser.add_argument(
        "--dependson",
        action="append",
        dest="depends_on",
        metavar="PKG",
        help="Rebuild packages that depend on PKG (can be repeated)",
    )
    rebuild_parser.add_argument(
        "--skippgpcheck", action="store_true", help="Skip PGP signature checks"
    )
    rebuild_parser.add_argument(
        "--skipinteg", action="store_true", help="Skip integrity checks"
    )
    rebuild_parser.add_argument(
        "--skipchecksums", action="store_true", help="Skip checksum checks"
    )

    args = parser.parse_args()

    # Set logging level based on verbose flag
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("Verbose logging enabled")

    # Acquire exclusive lock
    lock_fd = open(LOCK_FILE, "w")
    try:
        fcntl.flock(lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
    except BlockingIOError:
        logger.error("Another sol-repotool instance is running")
        sys.exit(1)

    match args.command:
        case "list":
            db_list(
                orphans=args.orphans,
                as_json=args.json,
                show_all=args.all,
                depends_on=args.depends_on,
            )
        case "prune":
            asyncio.run(db_prune(dry_run=args.dry_run))
        case "update":
            packages = args.package if args.package else None
            makepkg_flags = []
            if args.skippgpcheck:
                makepkg_flags.append("--skippgpcheck")
            if args.skipinteg:
                makepkg_flags.append("--skipinteg")
            if args.skipchecksums:
                makepkg_flags.append("--skipchecksums")
            asyncio.run(
                db_update(
                    packages=packages,
                    makepkg_flags=makepkg_flags if makepkg_flags else None,
                )
            )
        case "add":
            makepkg_flags = []
            if args.skippgpcheck:
                makepkg_flags.append("--skippgpcheck")
            if args.skipinteg:
                makepkg_flags.append("--skipinteg")
            if args.skipchecksums:
                makepkg_flags.append("--skipchecksums")
            asyncio.run(
                db_add(
                    args.package,
                    delete=args.rm,
                    as_deps=args.asdeps,
                    as_explicit=args.asexplicit,
                    as_noaur=args.noaur,
                    as_manual=args.manual,
                    as_no_manual=args.nomanual,
                    parents=args.parents,
                    makepkg_flags=makepkg_flags if makepkg_flags else None,
                )
            )
        case "remove":
            asyncio.run(db_remove(args.package))
        case "rebuild":
            makepkg_flags = []
            if args.skippgpcheck:
                makepkg_flags.append("--skippgpcheck")
            if args.skipinteg:
                makepkg_flags.append("--skipinteg")
            if args.skipchecksums:
                makepkg_flags.append("--skipchecksums")
            asyncio.run(
                db_rebuild(
                    packages=args.package if args.package else None,
                    makepkg_flags=makepkg_flags if makepkg_flags else None,
                    depends_on=args.depends_on,
                )
            )
        case "show-deps":
            db_show_deps(args.package if args.package else None)
        case "check-deps":
            db_check_deps()
        case "sign":
            asyncio.run(db_sign(all_pkgs=args.all))
        case "sync":
            asyncio.run(db_sync())
        case _:
            raise ValueError


if __name__ == "__main__":
    main()
