#!/usr/bin/env python3
import argparse
import asyncio
import dataclasses
import json
import os
import pathlib
import re
import shutil
import socket
import subprocess
import sys

import pyalpm

DEBUG = False

DATABASE = "sol"
REMOTE_HOST = "sol"  # The host where the repository lives

# Pre-compile regex for better performance when parsing many dependencies
VERSION_CONSTRAINT_RE = re.compile(r"[<>=]")


def is_remote_mode():
    """Check if we're running on a different host than the repository host."""
    return socket.gethostname() != REMOTE_HOST


async def remote_rsync_packages(package_files):
    """Rsync package files to the remote host.

    Args:
        package_files: List of package file paths to transfer

    Returns:
        List of remote file paths on the destination host
    """
    if not package_files:
        return []

    # Create temporary directory on remote host
    remote_tmp = f"/tmp/sol-repotool-{os.getpid()}"
    cmd = ["ssh", "-t", REMOTE_HOST, "mkdir", "-p", remote_tmp]
    proc = await asyncio.create_subprocess_exec(*cmd)
    returncode = await proc.wait()
    if returncode != 0:
        raise subprocess.CalledProcessError(returncode, cmd)

    # Rsync all packages
    for pkg_file in package_files:
        await run_cmd(
            "rsync",
            "-avz",
            "--progress",
            str(pkg_file),
            f"{REMOTE_HOST}:{remote_tmp}/",
            check=True,
        )

    # Return remote paths
    return [f"{remote_tmp}/{pathlib.Path(p).name}" for p in package_files]


async def remote_run_repotool(*args):
    """Run sol-repotool on the remote host via SSH.

    Args:
        *args: Arguments to pass to sol-repotool
    """
    cmd = ["ssh", "-t", REMOTE_HOST, "sol-repotool", *args]
    proc = await asyncio.create_subprocess_exec(*cmd)
    returncode = await proc.wait()
    if returncode != 0:
        raise subprocess.CalledProcessError(returncode, cmd)


def strip_version(dep_string):
    """Strip version information from a dependency string.

    Examples:
        "munt>=2.7.2" -> "munt"
        "foo=1.0" -> "foo"
        "bar: optional description" -> "bar"
    """
    # Handle optional dependency format with description
    dep_string = str(dep_string).split(":")[0].strip()
    # Strip version constraints
    return VERSION_CONSTRAINT_RE.split(dep_string)[0]


async def run_cmd(*cmd, check=False, capture_output=False, **kwargs):
    """
    Run a command using asyncio subprocess.

    Args:
        *cmd: Command and arguments
        check: If True, raise CalledProcessError on non-zero exit
        capture_output: If True, capture stdout/stderr
        **kwargs: Additional arguments for create_subprocess_exec

    Returns:
        CompletedProcess-like object with returncode, stdout, stderr
    """
    if capture_output:
        kwargs.setdefault("stdout", asyncio.subprocess.PIPE)
        kwargs.setdefault("stderr", asyncio.subprocess.PIPE)

    if DEBUG:
        print(f"Running command: {cmd} {kwargs}")
    proc = await asyncio.create_subprocess_exec(*cmd, **kwargs)
    stdout, stderr = await proc.communicate()

    if check and proc.returncode != 0:
        raise subprocess.CalledProcessError(
            proc.returncode, cmd, output=stdout, stderr=stderr
        )

    # Return a simple object with the same interface as subprocess.CompletedProcess
    class Result:
        def __init__(self, returncode, stdout, stderr):
            self.returncode = returncode
            self.stdout = stdout
            self.stderr = stderr

    return Result(proc.returncode, stdout, stderr)


@dataclasses.dataclass
class Package:
    name: str
    version: str
    depends: list
    optdepends: list
    makedepends: list
    provides: list
    pkgbase: str

    def __post_init__(self):
        """Strip version information from dependency fields."""
        self.depends = [strip_version(d) for d in self.depends]
        self.optdepends = [strip_version(d) for d in self.optdepends]
        self.makedepends = [strip_version(d) for d in self.makedepends]
        self.provides = [strip_version(p) for p in self.provides]

    @classmethod
    def from_alpm(cls, alpm_pkg):
        """Create a Package instance from an alpm.Package object.

        Args:
            alpm_pkg: A pyalpm.Package object

        Returns:
            Package instance with dependency versions stripped
        """
        return cls(
            name=alpm_pkg.name,
            version=alpm_pkg.version,
            depends=alpm_pkg.depends,
            optdepends=alpm_pkg.optdepends,
            makedepends=alpm_pkg.makedepends,
            provides=alpm_pkg.provides,
            pkgbase=alpm_pkg.base,
        )


class PackageDatabase:
    """Helper class to manage pyalpm database access."""

    OFFICIAL_REPOS = [
        "core",
        "extra",
        "multilib",
    ]

    def __init__(self, repo_name):
        self.repo_name = repo_name
        self.handle = pyalpm.Handle("/", "/var/lib/pacman")
        self._sol_db = self.handle.register_syncdb(self.repo_name, 0)

    @property
    def sol_db(self):
        """Get the sol repository database."""
        return self._sol_db

    def get_official_dbs(self):
        """Get official Arch repository databases (core, extra, multilib)."""
        for repo in self.OFFICIAL_REPOS:
            try:
                self.handle.register_syncdb(repo, 0)
            except Exception:
                pass  # Already registered
        return self.handle.get_syncdbs()

    def get_all_packages(self):
        """Get all packages in the sol repository."""
        packages = []
        for pkg in self.sol_db.pkgcache:
            packages.append(Package.from_alpm(pkg))
        return packages

    def get_package(self, name):
        """Get a specific package by name."""
        return self.sol_db.get_pkg(name)

    def load_package_file(self, file_path):
        """Load a package from a file."""
        return self.handle.load_pkg(str(file_path))


DOTFILES_PRIVATE = pathlib.Path(os.environ["DOTFILES_PRIVATE"])
CONFIG_PATH = DOTFILES_PRIVATE / "sol-repotool.json"


class ConfigFile:
    def __init__(self, path):
        self.path = pathlib.Path(path)
        self._load()

    def _load(self):
        self.aur = set()
        self.manual = set()
        if self.path.exists():
            data = json.loads(self.path.read_text())
            packages = data.get("packages", {})
            self.aur.update(packages.get("aur", []))
            self.manual.update(packages.get("manual", []))

    def _dump(self):
        self.path.parent.mkdir(parents=True, exist_ok=True)
        data = {
            "packages": {
                "aur": sorted(self.aur),
                "manual": sorted(self.manual),
            }
        }
        self.path.write_text(json.dumps(data, indent=2, sort_keys=True))

    def get_explicit_packages(self):
        return set(self.aur)

    def get_no_update_packages(self):
        return set(self.manual)

    def add_packages(self, *packages):
        updated = self.aur.copy()
        updated.update(packages)
        if updated != self.aur:
            self.aur = updated
            self._dump()

    def remove_packages(self, *packages):
        updated = self.aur - set(packages)
        if updated != self.aur:
            self.aur = updated
            self._dump()

    def add_no_update(self, *packages):
        updated = self.manual.copy()
        updated.update(packages)
        if updated != self.manual:
            self.manual = updated
            self._dump()

    def remove_no_update(self, *packages):
        updated = self.manual - set(packages)
        if updated != self.manual:
            self.manual = updated
            self._dump()


class Database:
    DIRECTORY_BASE = pathlib.Path("/var/cache/pacman")
    AURUTILS_CACHE = pathlib.Path.home() / ".cache/aurutils/sync"

    def __init__(
        self,
        name,
        /,
        aurutils_cache=None,
        directory=None,
    ):
        self.name = name

        self.directory = self.DIRECTORY_BASE / name
        if directory:
            self.directory = pathlib.Path(directory)

        self.aurutils_cache = self.AURUTILS_CACHE
        if aurutils_cache:
            self.aurutils_cache = pathlib.Path(aurutils_cache)

        self.config = ConfigFile(CONFIG_PATH)

        self.db_tar = self.directory / f"{self.name}.db.tar"
        self.db_tar_link = self.directory / f"{self.name}.db"
        self.files_tar = self.directory / f"{self.name}.files.tar"
        self.db_files_link = self.directory / f"{self.name}.files"

        self.pkg_db = PackageDatabase(self.name)

    async def _sign_database(self):
        tar_link = self.directory / f"{self.name}.db"
        files_link = self.directory / f"{self.name}.files"

        for path in [tar_link, files_link]:
            dest = path.with_name(path.name + ".sig")
            await run_cmd(
                "gpg",
                "--yes",
                "--detach-sign",
                "--use-agent",
                "--no-armor",
                "--output",
                str(dest),
                str(path),
                check=True,
            )

    async def cleanup(self):
        if not self.aurutils_cache.exists():
            return

        # Only clean up packages we're managing (config packages + their deps in sol repo)
        needed_packages = self._calculate_needed_packages()

        for pkg_name in needed_packages:
            pkg_cache = self.aurutils_cache / pkg_name
            if not pkg_cache.exists():
                continue

            for subpath in (pkg_cache / "src", pkg_cache / "pkg"):
                if not subpath.exists():
                    continue
                await run_cmd(
                    "chmod", "-R", "777", "--", str(subpath), check=True
                )
                try:
                    shutil.rmtree(subpath)
                except FileNotFoundError:
                    pass

    async def _get_dependencies(self, package_name, pkg_db=None):
        """
        Get all dependencies for a package.
        Returns a dict with 'required' and 'optional' lists.
        """
        # First try pyalpm if pkg_db is provided (much faster)
        if pkg_db:
            pkg = pkg_db.get_package(package_name)
            if pkg:
                required = []
                for dep in pkg.depends:
                    dep_name = strip_version(dep)
                    if dep_name and dep_name != package_name:
                        required.append(dep_name)

                optional = []
                for optdep in pkg.optdepends:
                    dep_name = strip_version(optdep)
                    if dep_name and dep_name != package_name:
                        optional.append(dep_name)

                return {
                    "required": sorted(required),
                    "optional": sorted(optional),
                }

        # Fallback to aur depends for packages not in sol database
        # Get required dependencies only
        proc_required = await run_cmd(
            "aur", "depends", "--", package_name, capture_output=True
        )

        required_deps = set()
        if proc_required.returncode == 0:
            for line in proc_required.stdout.decode().splitlines():
                parts = line.split()
                if len(parts) >= 2:
                    _, name = parts[0], parts[1]
                    if name != package_name:
                        required_deps.add(name)

        # Get all dependencies (required + optional)
        proc_all = await run_cmd(
            "aur",
            "depends",
            "--optdeps",
            "--",
            package_name,
            capture_output=True,
        )

        all_deps = set()
        if proc_all.returncode == 0:
            for line in proc_all.stdout.decode().splitlines():
                parts = line.split()
                if len(parts) >= 2:
                    _, name = parts[0], parts[1]
                    if name != package_name:
                        all_deps.add(name)

        # Optional deps are those in all_deps but not in required_deps
        optional_deps = all_deps - required_deps

        return {
            "required": sorted(list(required_deps)),
            "optional": sorted(list(optional_deps)),
        }

    async def _build_dependency_tree(self, packages, verbose=False):
        """
        Build a dependency tree for the given packages.
        Returns a dict mapping each package to its required and optional dependencies.
        Format: {"package_name": {"required": [...], "optional": [...]}}
        """
        # Initialize pyalpm to speed up dependency lookups
        dep_tree = {}

        for i, pkg in enumerate(packages, 1):
            if verbose:
                print(f"[{i}/{len(packages)}] Checking package: {pkg}")

            deps = await self._get_dependencies(pkg, pkg_db=self.pkg_db)

            if verbose:
                req_count = len(deps["required"])
                opt_count = len(deps["optional"])
                print(
                    f"  Found {req_count} required, {opt_count} optional dependencies"
                )

            dep_tree[pkg] = deps

        return dep_tree

    def package_names_from_json(self):
        """Get all package names from JSON file (packages + no_update)."""
        all_packages = self.config.get_explicit_packages()
        all_packages.update(self.config.get_no_update_packages())
        return list(all_packages)

    def packages(self):
        """Get all packages in the repository using pyalpm."""
        return self.pkg_db.get_all_packages()

    def _calculate_needed_packages(self):
        """
        Get all dependencies (depends, optdepends, makedepends) for a set of packages.
        Returns a set of package names including the input packages and all their deps.
        """
        all_packages = set(self.config.get_explicit_packages())

        # Process packages until we've expanded all dependencies
        to_process = all_packages.copy()
        processed = set()

        while to_process:
            pkg_name = to_process.pop()
            if pkg_name in processed:
                continue
            processed.add(pkg_name)

            pkg = self.pkg_db.get_package(pkg_name)
            if not pkg:
                continue

            # Extract and add all dependency types
            for dep in pkg.depends:
                dep_name = strip_version(dep)
                if dep_name and dep_name not in all_packages:
                    all_packages.add(dep_name)
                    to_process.add(dep_name)

            for optdep in pkg.optdepends:
                dep_name = strip_version(optdep)
                if dep_name and dep_name not in all_packages:
                    all_packages.add(dep_name)
                    to_process.add(dep_name)

            for makedep in pkg.makedepends:
                dep_name = strip_version(makedep)
                if dep_name and dep_name not in all_packages:
                    all_packages.add(dep_name)
                    to_process.add(dep_name)

        return all_packages

    def get_orphaned_packages(self):
        """
        Business Logic: Find packages that should be removed from the repository.

        Orphans = packages currently in repo - packages we need to keep

        A package is kept if:
        - Its name is needed, OR
        - Its pkgbase is needed, OR
        - It provides something that's needed
        """
        in_repo = {pkg.name for pkg in self.packages()}
        to_keep = self._calculate_needed_packages()
        orphans = set()

        for name in in_repo - to_keep:
            pkg = Package.from_alpm(self.pkg_db.get_package(name))

            # Check if package is needed by name, pkgbase, or any provides
            identifiers = [pkg.name, pkg.pkgbase, *pkg.provides]
            is_needed = any(
                identifier in to_keep for identifier in identifiers
            )

            if not is_needed:
                orphans.add(name)

        return orphans

    async def prune(self, dry_run=False):
        """Remove packages from repository that aren't in the JSON config."""
        to_remove = self.get_orphaned_packages()

        if to_remove:
            if dry_run:
                print(f"Would remove {len(to_remove)} packages not in config:")
                for package in sorted(to_remove):
                    print(f"  - {package}")
                return
            else:
                print(f"Removing {len(to_remove)} packages not in config:")
                for package in sorted(to_remove):
                    print(f"  - {package}")

                # Batch remove from database
                await run_cmd(
                    "repo-remove",
                    "--sign",
                    str(self.db_tar),
                    *sorted(to_remove),
                )

                # Remove package files and cache
                for package in to_remove:
                    # Remove package files
                    paths = (self.directory).glob(f"{package}*.pkg.*")
                    for path in paths:
                        path.unlink()

                    # Remove from cache
                    try:
                        shutil.rmtree(self.aurutils_cache / package)
                    except FileNotFoundError:
                        pass
        else:
            print("No packages to prune")
            return

        # Clean up cache for packages that are no longer needed
        needed_packages = self._calculate_needed_packages()

        for path in self.aurutils_cache.iterdir():
            if path.name not in needed_packages:
                print(f"Clearing cache for {path.name}")
                shutil.rmtree(path)

        await self._pacsync()
        await self._paccache()
        await self.sync_to_nullsum_net()

    async def sync_to_nullsum_net(self):
        await run_cmd(
            "rsync",
            "-avz",
            "--progress",
            "--delete-after",
            "/var/cache/pacman/sol/",
            "nullsum.net:~/srv/http/repo/arch/x86_64",
            check=True,
        )

    async def remove(self, *packages):
        packages_to_remove = self.config.aur.intersection(set(packages))
        if packages_to_remove:
            print(f"Removing {len(packages_to_remove)} package(s):")
            for pkg in sorted(packages_to_remove):
                print(f"  - {pkg}")

            self.config.remove_packages(*packages_to_remove)
            self.config.remove_no_update(*packages_to_remove)

            await run_cmd(
                "repo-remove",
                "--sign",
                str(self.db_tar),
                *sorted(packages_to_remove),
            )

            for package in packages_to_remove:
                paths = (self.directory).glob(f"{package}*.pkg.*")
                for path in paths:
                    path.unlink()
                try:
                    shutil.rmtree(self.aurutils_cache / package)
                except FileNotFoundError:
                    pass

        await self._pacsync()
        await self.sync_to_nullsum_net()

    @staticmethod
    async def _sign(path):
        path = pathlib.Path(path)
        dest = path.with_name(path.name + ".sig")
        await run_cmd(
            "gpg",
            "--yes",
            "--detach-sign",
            "--use-agent",
            "--no-armor",
            "--output",
            str(dest),
            str(path),
            check=True,
        )

    async def add(
        self, *packages, as_deps=False, as_explicit=False, parents=None
    ):
        # Build packages locally (same whether on sol or remote)
        if is_remote_mode():
            print(
                f"Running in remote mode (building on {socket.gethostname()}, adding to {REMOTE_HOST})"
            )

        # For remote mode, skip pacsync since we don't have the repo locally
        if not is_remote_mode():
            await self._pacsync()

        # Note: parents parameter is deprecated with the new list-based config format
        if parents:
            print(
                "Warning: --parent flag is deprecated and will be ignored",
                file=sys.stderr,
            )

        # Get all packages currently in the repository database (skip if remote)
        repo_packages = (
            set()
            if is_remote_mode()
            else {pkg.name for pkg in self.packages()}
        )

        packages_and_deps = set()
        new_packages = []
        pkg_files_to_transfer = []  # For remote mode
        already_exists = []

        reclassified = []

        for package in packages:
            maybe_path = pathlib.Path(package)
            if maybe_path.is_file():
                # Handle package file
                if is_remote_mode():
                    # In remote mode, just track file for transfer
                    pkg_files_to_transfer.append(maybe_path)
                else:
                    # Local mode: add directly to repository
                    await run_cmd(
                        "sudo",
                        "cp",
                        str(maybe_path),
                        str(self.directory),
                        check=True,
                    )
                    await run_cmd(
                        "repo-add",
                        str(self.db_tar),
                        str(maybe_path),
                        check=True,
                    )
                    await self._sign(self.directory / maybe_path.name)

                    # Extract package name and pkgbase using pyalpm
                    pkg_file = self.pkg_db.load_package_file(maybe_path)
                    pkg_name_only = pkg_file.name
                    pkgbase = pkg_file.base

                    self.config.add_no_update(pkgbase)

                    if not as_deps:
                        self.config.add_packages(pkgbase)
            else:
                # Check if package already exists in repository
                if package in repo_packages:
                    # Package is already built and in the repo
                    is_in_list = package in self.config.aur

                    if as_deps or as_explicit:
                        # User wants to change category
                        if as_deps and is_in_list:
                            # Change from explicit to dependency (remove from list)
                            self.config.remove_packages(package)
                            reclassified.append(
                                (package, "explicit", "dependency")
                            )
                        elif (as_explicit or (not as_deps)) and not is_in_list:
                            # Change to explicit (add to list)
                            self.config.add_packages(package)
                            reclassified.append(
                                (package, "dependency", "explicit")
                            )
                        else:
                            # Already in the correct category
                            already_exists.append(package)
                    else:
                        # Package exists and no category change requested
                        already_exists.append(package)
                    continue

                # Handle new AUR package
                # Just add the package itself - aur sync will handle dependencies
                # and makepkg/pacman will properly check if they're satisfied by provides
                packages_and_deps.add(package)
                new_packages.append(package)

                if not as_deps and not is_remote_mode():
                    self.config.add_packages(package)

        if packages_and_deps:
            await run_cmd(
                "aur",
                "sync",
                "--no-view",
                "--no-confirm",
                "--no-ver",
                f"--database={self.name}",
                "--",
                *packages_and_deps,
            )

            # If remote mode, find the built package files
            if is_remote_mode():
                for pkg_name in packages_and_deps:
                    # Look for .pkg.tar.* files in the repository directory
                    pkg_pattern = f"{pkg_name}*.pkg.tar.*"
                    matches = list(self.directory.glob(pkg_pattern))
                    # Filter out signature files
                    matches = [
                        m for m in matches if not m.name.endswith(".sig")
                    ]
                    if matches:
                        # Use the most recent match
                        pkg_file = max(
                            matches, key=lambda p: p.stat().st_mtime
                        )
                        pkg_files_to_transfer.append(pkg_file)

        # Report reclassified packages
        if reclassified:
            print(f"Reclassified {len(reclassified)} package(s):")
            for package, old_cat, new_cat in sorted(reclassified):
                print(f"  - {package}: {old_cat} â†’ {new_cat}")

        # Report packages that already exist
        if already_exists:
            print(
                f"Skipping {len(already_exists)} package(s) already in repository:"
            )
            for package in sorted(already_exists):
                print(f"  - {package}")

        # Remote mode: transfer packages and call sol-repotool add --rm
        if is_remote_mode():
            if not pkg_files_to_transfer:
                print("No packages to transfer")
                return

            # Transfer packages to sol
            print(
                f"Transferring {len(pkg_files_to_transfer)} package(s) to {REMOTE_HOST}..."
            )
            remote_paths = await remote_rsync_packages(pkg_files_to_transfer)

            # Build the add command
            add_args = ["add", "--rm"]
            if as_deps:
                add_args.append("--asdeps")
            if as_explicit:
                add_args.append("--asexplicit")
            add_args.extend(remote_paths)

            # Run sol-repotool add on the remote host
            print(f"Adding packages on {REMOTE_HOST}...")
            await remote_run_repotool(*add_args)
            print("Remote add completed successfully")
            return

        # Local mode: Exit early if nothing to add or reclassify
        if not new_packages and not reclassified:
            if already_exists:
                print("\nNo new packages to add.")
            return

        # Only sync if we actually built/added packages
        if new_packages:
            await self._pacsync()
            await self.sync_to_nullsum_net()
        elif reclassified:
            # Only update config file, no need to sync repo
            print("\nConfiguration updated (no repository sync needed)")

    async def update(self, list_only=False):
        # Remote mode: build locally, then transfer to sol
        if is_remote_mode():
            print(
                f"Running in remote mode (building on {socket.gethostname()}, updating on {REMOTE_HOST})"
            )
            # Get list of packages to update from remote
            print(f"Fetching package list from {REMOTE_HOST}...")

            # Use script to capture output while keeping interactive terminal
            import tempfile
            with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.txt') as f:
                tmpfile = f.name

            try:
                if DEBUG:
                    print(f"Running: ssh -t {REMOTE_HOST} sol-repotool update --list-only > {tmpfile}")
                proc = await asyncio.create_subprocess_exec(
                    "ssh", "-t", REMOTE_HOST, f"sol-repotool update --list-only > {tmpfile}",
                )
                await proc.wait()

                if proc.returncode != 0:
                    print("Failed to get package list from remote")
                    return

                # Fetch the file back
                if DEBUG:
                    print(f"Fetching: scp {REMOTE_HOST}:{tmpfile} {tmpfile}")
                proc2 = await asyncio.create_subprocess_exec(
                    "scp", f"{REMOTE_HOST}:{tmpfile}", tmpfile,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
                await proc2.wait()

                # Read the package list
                with open(tmpfile, 'r') as f:
                    package_list = [line.strip() for line in f if line.strip()]

                if DEBUG:
                    print(f"Got {len(package_list)} packages from remote")

                # Cleanup remote file
                if DEBUG:
                    print(f"Cleaning up: ssh {REMOTE_HOST} rm -f {tmpfile}")
                proc3 = await asyncio.create_subprocess_exec(
                    "ssh", REMOTE_HOST, f"rm -f {tmpfile}",
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
                await proc3.wait()
            finally:
                # Cleanup local file
                pathlib.Path(tmpfile).unlink(missing_ok=True)

            if not package_list:
                print("No packages to update")
                return

            print(f"Building {len(package_list)} package(s) locally:")
            for pkg in package_list:
                print(f"  - {pkg}")

            # Build packages locally
            proc = await asyncio.create_subprocess_exec(
                "aur",
                "sync",
                "--no-view",
                "--no-confirm",
                f"--database={self.name}",
                "--",
                *package_list,
            )
            returncode = await proc.wait()
            if returncode != 0:
                print("Build failed")
                return

            # Find built package files
            pkg_files_to_transfer = []
            for pkg_name in package_list:
                pkg_pattern = f"{pkg_name}*.pkg.tar.*"
                matches = list(self.directory.glob(pkg_pattern))
                matches = [m for m in matches if not m.name.endswith(".sig")]
                if matches:
                    pkg_file = max(matches, key=lambda p: p.stat().st_mtime)
                    pkg_files_to_transfer.append(pkg_file)

            if not pkg_files_to_transfer:
                print("No package files found to transfer")
                return

            # Transfer to sol and add
            print(f"Transferring {len(pkg_files_to_transfer)} package(s) to {REMOTE_HOST}...")
            remote_paths = await remote_rsync_packages(pkg_files_to_transfer)

            print(f"Adding packages on {REMOTE_HOST}...")
            await remote_run_repotool("add", "--rm", *remote_paths)
            print("Remote update completed successfully")
            return

        await self.cleanup()
        await self._pacsync()
        self.aurutils_cache.mkdir(exist_ok=True, parents=True)

        sync_dbs = self.pkg_db.get_official_dbs()

        packages_in_official_repos = set()
        repo_packages = {pkg.name for pkg in self.packages()}

        for pkg_name in repo_packages:
            # Check if package is now in any official repo
            for sync_db in sync_dbs:
                if sync_db.name != self.name and sync_db.get_pkg(pkg_name):
                    packages_in_official_repos.add(pkg_name)
                    break

        if packages_in_official_repos:
            print(
                f"Found {len(packages_in_official_repos)} packages now in official repos, removing from sol:"
            )
            for pkg in sorted(packages_in_official_repos):
                print(f"  - {pkg}")
            await self.remove(*packages_in_official_repos)

        # Calculate what packages we need
        needed_packages = self._calculate_needed_packages()

        # Get packages currently in the repo
        repo_packages = {pkg.name for pkg in self.packages()}

        # Only update packages that are currently in the repo and are AUR packages
        packages_to_update = (
            needed_packages & repo_packages
        ) - self.config.manual

        # Check for unsatisfied dependencies and add them to packages_to_update
        sync_dbs = self.pkg_db.get_official_dbs()

        missing_deps = set()

        for pkg_name in repo_packages:
            sol_pkg = self.pkg_db.get_package(pkg_name)
            if not sol_pkg:
                continue

            # Check only required dependencies
            for dep in sol_pkg.depends:
                dep_name = strip_version(dep)

                if not dep_name:
                    continue

                satisfied = False

                # Check sol repo
                if self.pkg_db.get_package(dep_name):
                    satisfied = True
                else:
                    # Check provides in sol repo
                    for p in self.pkg_db.sol_db.pkgcache:
                        for prov in p.provides:
                            prov_name = strip_version(prov)
                            if prov_name == dep_name:
                                satisfied = True
                                break
                        if satisfied:
                            break

                # Check sync repos (official repos)
                if not satisfied:
                    for sync_db in sync_dbs:
                        if sync_db.get_pkg(dep_name):
                            satisfied = True
                            break
                        # Check provides in sync repos
                        for p in sync_db.pkgcache:
                            for prov in p.provides:
                                prov_name = strip_version(prov)
                                if prov_name == dep_name:
                                    satisfied = True
                                    break
                            if satisfied:
                                break
                        if satisfied:
                            break

                if not satisfied:
                    missing_deps.add(dep_name)

        if missing_deps:
            print(
                f"Found {len(missing_deps)} missing dependencies, adding to update list:"
            )
            for dep in sorted(missing_deps):
                print(f"  - {dep}")
            packages_to_update.update(missing_deps)

        if not packages_to_update:
            print("No AUR packages to update")
            return

        # Calculate maximum distance from parents for each package
        # This ensures dependencies are updated before packages that depend on them
        distances = {}
        parents = self.config.aur

        # Create pyalpm handle once for efficiency
        try:
            sol_db = self.pkg_db.sol_db
        except Exception:
            raise
            # If we can't get the database, just use unsorted packages
            sorted_packages = list(packages_to_update)
            await run_cmd(
                "aur",
                "sync",
                "--no-view",
                "--no-confirm",
                f"--database={self.name}",
                "--",
                *sorted_packages,
                cwd=str(self.aurutils_cache),
                check=True,
            )
            await self._pacsync()
            await self._paccache()
            await self.sync_to_nullsum_net()
            return

        def calculate_distance(pkg, visited=None):
            if visited is None:
                visited = set()
            if pkg in visited:
                return 0
            if pkg in distances:
                return distances[pkg]

            visited.add(pkg)

            # If it's a parent, distance is 0
            if pkg in parents:
                distances[pkg] = 0
                return 0

            # Find all packages that depend on this package
            max_dist = 0

            # Check which packages depend on this one
            for potential_parent in packages_to_update:
                parent_pkg = sol_db.get_pkg(potential_parent)
                if parent_pkg:
                    # Check if pkg is in this package's dependencies
                    for dep in parent_pkg.depends:
                        dep_name = strip_version(dep)
                        if dep_name == pkg:
                            dist = (
                                calculate_distance(
                                    potential_parent, visited.copy()
                                )
                                + 1
                            )
                            max_dist = max(max_dist, dist)
                    for optdep in parent_pkg.optdepends:
                        dep_name = strip_version(optdep)
                        if dep_name == pkg:
                            dist = (
                                calculate_distance(
                                    potential_parent, visited.copy()
                                )
                                + 1
                            )
                            max_dist = max(max_dist, dist)

            distances[pkg] = max_dist
            return max_dist

        # Calculate distances for all packages
        for pkg in packages_to_update:
            calculate_distance(pkg)

        # Sort packages by distance (highest first - deepest dependencies first)
        # Ensure each package appears only once
        seen = set()
        sorted_packages = []
        for pkg in sorted(
            packages_to_update, key=lambda p: distances.get(p, 0), reverse=True
        ):
            if pkg not in seen:
                seen.add(pkg)
                sorted_packages.append(pkg)

        # If list_only, just print packages and exit
        if list_only:
            for pkg in sorted_packages:
                print(pkg)
            return

        # Sync all AUR packages (this will run srcver and detect git package updates)
        await run_cmd(
            "aur",
            "sync",
            "--no-view",
            "--no-confirm",
            f"--database={self.name}",
            "--",
            *sorted_packages,
            cwd=str(self.aurutils_cache),
            check=True,
        )
        await self._pacsync()
        await self._paccache()
        await self.sync_to_nullsum_net()
        return

    async def _paccache(self):
        await run_cmd(
            "paccache",
            "--remove",
            "--keep",
            "1",
            "--cachedir",
            str(self.directory),
            check=True,
        )

    async def _pacsync(self):
        proc = await asyncio.create_subprocess_exec("sudo", "pacsync", self.name)
        returncode = await proc.wait()
        if returncode != 0:
            raise subprocess.CalledProcessError(returncode, ["sudo", "pacsync", self.name])


def db_list(orphans=False, as_json=False, show_all=False):
    database = Database(DATABASE)

    if orphans:
        # Show orphaned packages
        orphan_list = database.get_orphaned_packages()
        if orphan_list:
            for package in sorted(orphan_list):
                print(package)
        return

    if as_json:
        # Output full package info as JSON for packages in config
        try:
            pkg_db = PackageDatabase(DATABASE)
        except Exception as e:
            print(f"Error: Coul not access database: {e}", file=sys.stderr)
            return

        # Get packages to show
        if show_all:
            # Show all packages in sol database
            packages_to_show = [pkg.name for pkg in database.packages()]
        else:
            # Show only packages from config
            packages_to_show = list(database.config.get_explicit_packages())

        result = {}
        for pkg_name in packages_to_show:
            pkg = pkg_db.get_package(pkg_name)
            if not pkg:
                continue

            # Get dependencies (only those in sol)
            depends = []
            for dep in pkg.depends:
                dep_name = strip_version(dep)
                if dep_name and pkg_db.get_package(dep_name):
                    depends.append(dep_name)

            # Get optional dependencies (only those in sol)
            optdepends = []
            for optdep in pkg.optdepends:
                dep_name = strip_version(optdep)
                if dep_name and pkg_db.get_package(dep_name):
                    optdepends.append(dep_name)

            # Get makedepends (only those in sol)
            makedepends = []
            for makedep in pkg.makedepends:
                dep_name = strip_version(makedep)
                if dep_name and pkg_db.get_package(dep_name):
                    makedepends.append(dep_name)

            pkg_info = {}
            if depends:
                pkg_info["depends"] = sorted(depends)
            if optdepends:
                pkg_info["optdepends"] = sorted(optdepends)
            if makedepends:
                pkg_info["makedepends"] = sorted(makedepends)

            result[pkg_name] = pkg_info

        print(json.dumps(result, indent=2))
        return

    # Default: list package names
    for package in database.packages():
        print(package.name)


def db_show_deps(packages=None):
    """Show dependencies for packages in the sol database as JSON."""
    database = Database(DATABASE)

    # Get list of packages to query
    if packages:
        package_list = set(packages)
    else:
        # Get all packages from the database
        package_list = {pkg.name for pkg in database.packages()}

    if not package_list:
        print("{}")
        return

    # Initialize pyalpm
    pkg_db = PackageDatabase(DATABASE)

    result = {"packages": {}}

    # Query each package
    for pkg_name in package_list:
        pkg = pkg_db.get_package(pkg_name)
        if not pkg:
            continue

        # Get required dependencies
        required = []
        for dep in pkg.depends:
            dep_name = strip_version(dep)
            if dep_name:
                required.append(dep_name)

        # Get optional dependencies
        optional = []
        for optdep in pkg.optdepends:
            dep_name = strip_version(optdep)
            if dep_name:
                optional.append(dep_name)

        result["packages"][pkg_name] = {
            "required": required,
            "optional": optional,
        }

    print(json.dumps(result, indent=2))


def db_check_deps():
    """Check for unsatisfied dependencies in the repo."""
    database = Database(DATABASE)

    try:
        pkg_db = PackageDatabase(DATABASE)
        sync_dbs = pkg_db.get_official_dbs()

        unsatisfied = {}

        for pkg in database.packages():
            sol_pkg = pkg_db.get_package(pkg.name)
            if not sol_pkg:
                continue

            pkg_unsatisfied = []

            # Check only required dependencies (not optdepends or makedepends)
            all_deps = list(sol_pkg.depends)

            for dep in all_deps:
                # Extract dependency name (strip version constraints)
                dep_name = strip_version(dep)

                if not dep_name:
                    continue

                satisfied = False

                # Check sol repo
                if pkg_db.get_package(dep_name):
                    satisfied = True
                else:
                    # Check provides in sol repo
                    for p in pkg_db.sol_db.pkgcache:
                        for prov in p.provides:
                            prov_name = strip_version(prov)
                            if prov_name == dep_name:
                                satisfied = True
                                break
                        if satisfied:
                            break

                # Check sync repos (official repos)
                if not satisfied:
                    for sync_db in sync_dbs:
                        if sync_db.get_pkg(dep_name):
                            satisfied = True
                            break
                        # Check provides in sync repos
                        for p in sync_db.pkgcache:
                            for prov in p.provides:
                                prov_name = strip_version(prov)
                                if prov_name == dep_name:
                                    satisfied = True
                                    break
                            if satisfied:
                                break
                        if satisfied:
                            break

                if not satisfied:
                    pkg_unsatisfied.append(dep_name)

            if pkg_unsatisfied:
                unsatisfied[pkg.name] = sorted(set(pkg_unsatisfied))

        if unsatisfied:
            print("Packages with unsatisfied dependencies:")
            for pkg_name, deps in sorted(unsatisfied.items()):
                print(f"\n{pkg_name}:")
                for dep in deps:
                    print(f"  - {dep}")
            return 1
        else:
            print("All dependencies are satisfied")
            return 0

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        import traceback

        traceback.print_exc()
        return 1


async def db_prune(dry_run=False):
    database = Database(DATABASE)
    await database.prune(dry_run=dry_run)


async def db_sync():
    database = Database(DATABASE)
    await database.sync_to_nullsum_net()


async def db_remove(packages):
    database = Database(DATABASE)
    await database.remove(*packages)


async def db_add(
    packages, delete=False, as_deps=False, as_explicit=False, parents=None
):
    database = Database(DATABASE)
    await database.add(
        *packages, as_deps=as_deps, as_explicit=as_explicit, parents=parents
    )
    if delete:
        for package in packages:
            path = pathlib.Path(package)
            if path.is_file():
                path.unlink()


async def db_update(list_only=False):
    database = Database(DATABASE)
    await database.update(list_only=list_only)


async def db_sign(all=False):
    database = Database(DATABASE)
    if all:
        for path in database.directory.iterdir():
            if not path.name.endswith(".pkg.tar.zst"):
                continue
            await database._sign(path)
    # database._sign_database()
    await database.sync_to_nullsum_net()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--verbose", action="store_true")
    subparsers = parser.add_subparsers(
        dest="command", metavar="COMMAND", required=True
    )

    add_parser = subparsers.add_parser(
        "add", help="Add packages to the repository"
    )
    add_parser.add_argument(
        "--rm", action="store_true", help="Delete package file after adding"
    )
    add_parser.add_argument(
        "--asdeps", action="store_true", help="Add as dependency"
    )
    add_parser.add_argument(
        "--asexplicit", action="store_true", help="Add as explicit (default)"
    )
    add_parser.add_argument(
        "--parent",
        action="append",
        dest="parents",
        help="Add as dependency of parent package (can be specified multiple times)",
    )
    add_parser.add_argument("package", nargs="+")

    list_parser = subparsers.add_parser(
        "list", help="List packages in the repository"
    )
    list_parser.add_argument(
        "-o",
        "--orphans",
        action="store_true",
        help="Show orphaned packages only",
    )
    list_parser.add_argument(
        "--json",
        action="store_true",
        help="Output package dependencies as JSON",
    )
    list_parser.add_argument(
        "-a",
        "--all",
        action="store_true",
        help="Include all dependencies (not just sol packages)",
    )

    prune_parser = subparsers.add_parser(
        "prune", help="Remove packages not in config"
    )
    prune_parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be removed without removing",
    )

    show_deps_parser = subparsers.add_parser(
        "show-deps", help="Show dependencies as JSON"
    )
    show_deps_parser.add_argument(
        "package",
        nargs="*",
        help="Specific packages (optional, defaults to all)",
    )

    subparsers.add_parser(
        "check-deps", help="Check for unsatisfied dependencies"
    )

    remove_parser = subparsers.add_parser(
        "remove", help="Remove packages from the repository"
    )
    remove_parser.add_argument("package", nargs="+")

    sign_parser = subparsers.add_parser("sign", help="Sign packages")
    sign_parser.add_argument("--all", action="store_true")

    subparsers.add_parser("sync", help="Sync repository to remote")

    update_parser = subparsers.add_parser("update", help="Update all packages")
    update_parser.add_argument(
        "--list-only",
        action="store_true",
        help="Only list packages to update (don't build)",
    )

    args = parser.parse_args()

    if args.verbose:
        global DEBUG
        DEBUG = True

    match args.command:
        case "list":
            db_list(orphans=args.orphans, as_json=args.json, show_all=args.all)
        case "prune":
            asyncio.run(db_prune(dry_run=args.dry_run))
        case "update":
            asyncio.run(db_update(list_only=args.list_only))
        case "add":
            asyncio.run(
                db_add(
                    args.package,
                    delete=args.rm,
                    as_deps=args.asdeps,
                    as_explicit=args.asexplicit,
                    parents=args.parents,
                )
            )
        case "remove":
            asyncio.run(db_remove(args.package))
        case "show-deps":
            db_show_deps(args.package if args.package else None)
        case "check-deps":
            db_check_deps()
        case "sign":
            asyncio.run(db_sign(all=args.all))
        case "sync":
            asyncio.run(db_sync())
        case _:
            raise ValueError


if __name__ == "__main__":
    main()
